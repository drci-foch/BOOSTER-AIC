{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "102557f9-7184-484b-bd0b-79795636e49b",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7759bce3-13cf-4cda-a6e8-1f01f86b305c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wijflo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchio as tio\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import segmentation_models_pytorch_3d as smp3d\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b2901c-63ed-44ae-88bd-8bdbaacd7bb4",
   "metadata": {},
   "source": [
    "## Filter out problematic images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d8537031-08db-41d9-8c64-d1fb376c5583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['06-10664-D0MR']\n",
      "['09-10683-D0MR']\n",
      "['09-10890-D0MR']\n",
      "['16-10232-D0MR']\n",
      "['21-10049-D0MR']\n",
      "['21-10049-D0MR']\n"
     ]
    }
   ],
   "source": [
    "source_dir = \"D:\\\\THROMBMICS-ALARMS_20240531\\\\SWI\"\n",
    "problem_dir = \"D:\\\\THROMBMICS-ALARMS_20240531\\\\Problem_Images\\\\SWI\"\n",
    "prev = []\n",
    "for file in os.listdir(source_dir):\n",
    "    number = file.split(\"_\")[1:2]\n",
    "    # if file.split(\"_\")[-1] == \"ph.nii.gz\":\n",
    "    #     ph_q = True\n",
    "    # else:\n",
    "    #     ph_q = False\n",
    "    if (number == prev):\n",
    "        print(number)\n",
    "        #os.rename(os.path.join(source_dir, file), os.path.join(problem_dir, file))\n",
    "    prev = number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "987fa1f8-78da-41d5-a7a2-da18b8b95351",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = \"D:\\\\THROMBMICS-ALARMS_20240531\\\\TOF3D\"\n",
    "problem_dir = \"D:\\\\THROMBMICS-ALARMS_20240531\\\\Problem_Images\\\\TOF3D\"\n",
    "prev = []\n",
    "for file in os.listdir(source_dir):\n",
    "    number = file.split(\"_\")[1:2]\n",
    "    if \"_\".join(file.split(\"_\")[-2:]) == \"Eq_1.nii.gz\":\n",
    "        eq1_q = True\n",
    "    else:\n",
    "        eq1_q = False\n",
    "    if (number == prev) & (eq1_q):\n",
    "        os.rename(os.path.join(source_dir, file), os.path.join(problem_dir, file))\n",
    "    prev = number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc631a49-7746-496f-9fb5-2e925805c890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'16-10170-D0MR', '02-10871-D0MR', '07-10333-D0MR', '14-10034-D0MR', '21-10163-D0MR', '21-10135-D0MR', '18-10428-D0MR', '01-10221-D0MR', '14-10119-D0MR', '14-10239-D0MR', '06-10750-D0MR', '05-10410-D0MR', '30-10034-D0MR', '18-10183-D0MR', '14-10164-D0MR', '30-10085-D0MR', '06-10487-D0MR', '14-10269-D0MR', '14-10115-D0MR', '18-10542-D0MR', '18-10099-D0MR', '06-10516-D0MR', '09-10890-D0MR', '21-10158-D0MR', '02-10874-D0MR', '30-10091-D0MR', '16-10168-D0MR', '30-10092-D0MR', '30-10090-D0MR', '06-10778-D0MR', '14-10156-D0MR', '02-10555-D0MR', '17-10120-D0MR', '16-10025-D0MR', '14-10172-D0MR', '02-10878-D0MR', '14-10068-D0MR', '06-10769-D0MR', '02-10722-D0MR', '14-10153-D0MR', '14-10238-D0MR', '30-10082-D0MR', '30-10083-D0MR', '07-10335-D0MR', '14-10120-D0MR', '30-10076-D0MR', '18-10396-D0MR', '14-10123-D0MR', '18-10206-D0MR', '14-10173-D0MR', '14-10087-D0MR', '21-10049-D0MR', '30-10088-D0MR', '14-10166-D0MR', '04-10442-D0MR', '14-10125-D0MR', '30-10094-D0MR', '14-10243-D0MR', '09-10674-D0MR', '09-10683-D0MR', '09-10670-D0MR', '30-10068-D0MR', '02-10652-D0MR', '30-10089-D0MR', '02-10653-D0MR', '18-10315-D0MR', '14-10171-D0MR', '04-10209-D0MR', '05-10383-D0MR', '30-10084-D0MR', '18-10037-D0MR', '11-10071-D0MR', '16-10117-D0MR', '18-10150-D0MR', '14-10086-D0MR', '30-10070-D0MR'}\n"
     ]
    }
   ],
   "source": [
    "swi_dir = \"D:\\\\THROMBMICS-ALARMS_20240531\\\\SWI\"\n",
    "mask_dir = \"D:\\\\THROMBMICS-ALARMS_20240531\\\\MASK\"\n",
    "\n",
    "swi_numbers = [file.split(\"_\")[1:2][0] for file in os.listdir(swi_dir)]\n",
    "mask_numbers = [file.split(\"_\")[1:2][0] for file in os.listdir(mask_dir)]\n",
    "\n",
    "diff = set(mask_numbers) - set(swi_numbers)\n",
    "\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "473d4408-8c01-425e-80ba-9a1b0bb3f0c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2018-104_16-10170-D0MR_22_AX_T2_EG.nii.gz\n",
      "Processed 2018-104_02-10871-D0MR_6_Ax_T2_GRE.nii.gz\n",
      "Processed 2018-104_07-10333-D0MR_20_SWI_Images.nii.gz\n",
      "Processed 2018-104_14-10034-D0MR_5_Ax_T2_.nii.gz\n",
      "Processed 2018-104_21-10163-D0MR_401_cs_T2_FFE.nii.gz\n",
      "Processed 2018-104_21-10135-D0MR_5_Ax_T2_.nii.gz\n",
      "Processed 2018-104_18-10428-D0MR_11_T2_EG_AX.nii.gz\n",
      "Processed 2018-104_01-10221-D0MR_11_SWI_Images.nii.gz\n",
      "Processed 2018-104_14-10119-D0MR_16_T2_EG_AX.nii.gz\n",
      "Processed 2018-104_14-10239-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_06-10750-D0MR_8_AX_T2_EG_STD.nii.gz\n",
      "Processed 2018-104_05-10410-D0MR_12_t2_fl2d_tra_4mm_hemo_te_25.nii.gz\n",
      "Processed 2018-104_30-10034-D0MR_8_t2_fl2d_ax.nii.gz\n",
      "Processed 2018-104_18-10183-D0MR_12_T2_EG_AX.nii.gz\n",
      "Processed 2018-104_14-10164-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_30-10085-D0MR_11_SWI_Images.nii.gz\n",
      "Processed 2018-104_06-10487-D0MR_9_AX_T2_EG_STD.nii.gz\n",
      "Processed 2018-104_14-10269-D0MR_16_AX_T2_.nii.gz\n",
      "Processed 2018-104_14-10115-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_18-10542-D0MR_8_AXIAL_T2_EG.nii.gz\n",
      "Processed 2018-104_18-10099-D0MR_5_Ax_T2_4mm.nii.gz\n",
      "Processed 2018-104_06-10516-D0MR_502_eAX_T2_.nii.gz\n",
      "Processed 2018-104_09-10890-D0MR_501_SWI_AVC.nii.gz\n",
      "Processed 2018-104_09-10890-D0MR_501_SWI_AVC_ph.nii.gz\n",
      "Processed 2018-104_21-10158-D0MR_401_cs_T2_FFE.nii.gz\n",
      "Processed 2018-104_02-10874-D0MR_6_Ax_eSWAN_2017.nii.gz\n",
      "Processed 2018-104_30-10091-D0MR_11_SWI_Images.nii.gz\n",
      "Processed 2018-104_16-10168-D0MR_15_SWI_Images.nii.gz\n",
      "Processed 2018-104_30-10092-D0MR_9_SWI_Images.nii.gz\n",
      "Processed 2018-104_30-10090-D0MR_11_SWI_Images.nii.gz\n",
      "Processed 2018-104_06-10778-D0MR_4_Ax_T2_GRE.nii.gz\n",
      "Processed 2018-104_14-10156-D0MR_6_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_02-10555-D0MR_5_3D_eSWAN_RAPIDE.nii.gz\n",
      "Processed 2018-104_17-10120-D0MR_6_Ax_T2_.nii.gz\n",
      "Processed 2018-104_16-10025-D0MR_12_T2_EG_AX.nii.gz\n",
      "Processed 2018-104_14-10172-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_02-10878-D0MR_5_Ax_T2_GRE.nii.gz\n",
      "Processed 2018-104_14-10068-D0MR_5_Ax_T2_.nii.gz\n",
      "Processed 2018-104_06-10769-D0MR_13_AX_T2_EG_STD.nii.gz\n",
      "Processed 2018-104_02-10722-D0MR_6_Ax_3D_SWAN+CARTO.nii.gz\n",
      "Processed 2018-104_14-10153-D0MR_5_3D_Ax_SWAN_4mm.nii.gz\n",
      "Processed 2018-104_14-10238-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_30-10082-D0MR_11_SWI_Images.nii.gz\n",
      "Processed 2018-104_30-10083-D0MR_13_SWI_Images.nii.gz\n",
      "Processed 2018-104_07-10335-D0MR_6_Ax_T2_GRE.nii.gz\n",
      "Processed 2018-104_14-10120-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_30-10076-D0MR_8_t2_fl2d_ax.nii.gz\n",
      "Processed 2018-104_18-10396-D0MR_12_T2_EG_AX.nii.gz\n",
      "Processed 2018-104_14-10123-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_18-10206-D0MR_11_T2_EG_AX.nii.gz\n",
      "Processed 2018-104_14-10173-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_14-10087-D0MR_5_Ax_SWAN_2.4MM.nii.gz\n",
      "Processed 2018-104_21-10049-D0MR_401_SWIp.nii.gz\n",
      "Processed 2018-104_21-10049-D0MR_401_SWIp_Eq_1.nii.gz\n",
      "Processed 2018-104_21-10049-D0MR_401_SWIp_ph.nii.gz\n",
      "Processed 2018-104_30-10088-D0MR_11_SWI_Images.nii.gz\n",
      "Processed 2018-104_14-10166-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_04-10442-D0MR_14_T2_EG_TRA.nii.gz\n",
      "Processed 2018-104_14-10125-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_30-10094-D0MR_4_Ax_T2_GRE.nii.gz\n",
      "Processed 2018-104_14-10243-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_09-10674-D0MR_5_Ax_T2_GRE.nii.gz\n",
      "Processed 2018-104_09-10683-D0MR_501_SWI_AVC.nii.gz\n",
      "Processed 2018-104_09-10683-D0MR_501_SWI_AVC_ph.nii.gz\n",
      "Processed 2018-104_09-10670-D0MR_502_AXIAL_SWI.nii.gz\n",
      "Processed 2018-104_30-10068-D0MR_8_t2_fl2d_ax.nii.gz\n",
      "Processed 2018-104_02-10652-D0MR_5_Ax_SWAN.nii.gz\n",
      "Processed 2018-104_30-10089-D0MR_15_SWI_Images.nii.gz\n",
      "Processed 2018-104_02-10653-D0MR_13_AX_T2_EG.nii.gz\n",
      "Processed 2018-104_18-10315-D0MR_11_T2_EG_AX.nii.gz\n",
      "Processed 2018-104_14-10171-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_04-10209-D0MR_5_Ax_T2_GRE.nii.gz\n",
      "Processed 2018-104_05-10383-D0MR_11_SWI_Images.nii.gz\n",
      "Processed 2018-104_30-10084-D0MR_11_SWI_Images.nii.gz\n",
      "Processed 2018-104_18-10037-D0MR_501_T2_FFE_CS.nii.gz\n",
      "Processed 2018-104_11-10071-D0MR_11_SWI_Images.nii.gz\n",
      "Processed 2018-104_16-10117-D0MR_13_T2_EG_AX.nii.gz\n",
      "Processed 2018-104_18-10150-D0MR_14_T2_EG_AX.nii.gz\n",
      "Processed 2018-104_14-10086-D0MR_6_3D_Ax_SWAN.nii.gz\n",
      "Processed 2018-104_30-10070-D0MR_8_t2_fl2d_ax.nii.gz\n"
     ]
    }
   ],
   "source": [
    "source_dir = \"E:\\\\Data_ETIS\\\\THROMBMICS-ALARMS_20240531\"\n",
    "target_dir = \"E:\\\\Data_ETIS\\\\Temp\"\n",
    "\n",
    "for number in list(diff):\n",
    "    for directory in glob.glob(os.path.join(source_dir, \"2018-104_\"+ number, \"T2star_*\")):\n",
    "        for nii_file in os.listdir(directory):\n",
    "            os.rename(os.path.join(directory, nii_file), os.path.join(target_dir, nii_file))\n",
    "            print(\"Processed \"+ nii_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b611b762-ea53-415a-af59-a47a1073ae11",
   "metadata": {},
   "source": [
    "## Separate Test Batch of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ffd9f8-2f8d-4b13-833a-02053ebea35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Train_labels\\\\MASK_Train\"\n",
    "mask_test_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Test_labels\\\\MASK_Test\"\n",
    "mask_val_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_labels\\\\MASK_Val\"\n",
    "swi_train_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Train_dataset\\\\SWI_Train\"\n",
    "swi_test_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Test_dataset\\\\SWI_Test\"\n",
    "swi_val_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_dataset\\\\SWI_Val\"\n",
    "tof_train_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Train_dataset\\\\TOF3D_Train\"\n",
    "tof_test_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Test_dataset\\\\TOF3D_Test\"\n",
    "tof_val_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_dataset\\\\TOF3D_Val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15cfda5e-2785-4836-ae83-889a9c3680ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_image_batch(mask_source, mask_destination, swi_source, swi_destination, tof_source, tof_destination, batch_size, seed_value=777):\n",
    "    # Separate three source folders, mask(labels), swi images, tof images into three other destination folders (e.g. validation or test),\n",
    "    # sending the specified number of images selected randomly.\n",
    "    random.seed(seed_value)\n",
    "    batch_indexes = random.sample(range(len(os.listdir(mask_source))), batch_size)\n",
    "\n",
    "    mask_file_list = [os.listdir(mask_source)[index] for index in batch_indexes]\n",
    "    for file in mask_file_list:\n",
    "        os.rename(os.path.join(mask_source, file), os.path.join(mask_destination, file))\n",
    "\n",
    "    swi_file_list = [os.listdir(swi_source)[index] for index in batch_indexes]\n",
    "    for file in swi_file_list:\n",
    "        os.rename(os.path.join(swi_source, file), os.path.join(swi_destination, file))\n",
    "\n",
    "    tof_file_list = [os.listdir(tof_source)[index] for index in batch_indexes]\n",
    "    for file in tof_file_list:\n",
    "        os.rename(os.path.join(tof_source, file), os.path.join(tof_destination, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0e1d3c-9c23-403f-8526-00efb1ca3d63",
   "metadata": {},
   "source": [
    "Separate test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "626d9491-7bfb-4bd3-9313-97dbe1b65efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_image_batch(mask_train_dir, mask_test_dir, swi_train_dir, swi_test_dir, tof_train_dir, tof_test_dir, 100, seed_value=777)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2501cdcb-4fc7-469c-bfe0-68f0386c28d6",
   "metadata": {},
   "source": [
    "Separate validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9ceb8cf-3ec5-480f-89c3-db6377789c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_image_batch(mask_train_dir, mask_val_dir, swi_train_dir, swi_val_dir, tof_train_dir, tof_val_dir, 181, seed_value=777)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192edc50-ce79-4d19-92b1-9c341d8a235c",
   "metadata": {},
   "source": [
    "Clear out the training folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a6f313a-6428-4483-930a-0dd5d31811e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_training_folders(mask_train_dir, mask_test_dir, mask_val_dir, swi_train_dir, swi_test_dir, swi_val_dir, tof_train_dir, tof_test_dir, tof_val_dir):\n",
    "    # Remove files from training, validation and test folders of masks, swi images and tof images.\n",
    "    folder_list = [mask_train_dir, mask_test_dir, mask_val_dir, swi_train_dir, swi_test_dir, swi_val_dir, tof_train_dir, tof_test_dir, tof_val_dir]\n",
    "    for folders in folder_list:\n",
    "        for file in os.listdir(folders):\n",
    "            os.remove(os.path.join(folders, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc04a0dc-cf1a-4b8c-af61-c0d3111a7a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_training_folders(mask_train_dir, mask_test_dir, mask_val_dir, swi_train_dir, swi_test_dir, swi_val_dir, tof_train_dir, tof_test_dir, tof_val_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f50055-f8bf-4c98-be7f-72b256862585",
   "metadata": {},
   "source": [
    "Fill training folders from processed images folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d97d0b10-438e-4df8-a04c-3ae0c5b33db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_training_folders(source_dir, mask_train_dir, swi_train_dir, tof_train_dir):\n",
    "    # Send images from a source folder containing MASK, SWI and TOF3D folders to the training folders.\n",
    "    for folders in os.listdir(source_dir):\n",
    "        if (folders.split(\"_\")[0] == \"MASK\") | (folders == \"MASK\"):\n",
    "            for files in os.listdir(os.path.join(source_dir, folders)):\n",
    "                os.rename(os.path.join(source_dir, folders, files), os.path.join(mask_train_dir,files))\n",
    "        elif folders.split(\"_\")[0] == \"SWI\":\n",
    "            for files in os.listdir(os.path.join(source_dir, folders)):\n",
    "                os.rename(os.path.join(source_dir, folders, files), os.path.join(swi_train_dir,files))\n",
    "        elif folders.split(\"_\")[0] == \"TOF3D\":\n",
    "            for files in os.listdir(os.path.join(source_dir, folders)):\n",
    "                os.rename(os.path.join(source_dir, folders, files), os.path.join(tof_train_dir,files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "245da8d3-712e-4e64-ae11-09e365713ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = \"D:\\\\data_ETIS_781\\\\Resized\"\n",
    "\n",
    "fill_training_folders(source_dir, mask_train_dir, swi_train_dir, tof_train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7944ba3-9b36-425b-a661-8cbbfd62772b",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "870e295a-7037-4fe4-9bfc-9e00b95faaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_padding_2D(slice_tensor, original_height, original_width):\n",
    "    # Remove padding from a 2D tensor, returning it to specified dimensions. The padding is removed from the right and from the bottom.\n",
    "    return slice_tensor[:original_height, :original_height]\n",
    "\n",
    "def remove_padding_from_tensor(tensor, original_height, original_width):\n",
    "    # Apply the padding removal function to the tensor and restack the slices to form 3D images.\n",
    "    unpadded_slices = []\n",
    "\n",
    "    for i in range(tensor.shape[0]):\n",
    "        unpadded_slice=[]\n",
    "        for j in range(tensor.shape[3]):\n",
    "            slice_tensor = tensor[i, :, :, j]\n",
    "            unpadded_slice.append(remove_padding(slice_tensor, original_height, original_width))\n",
    "        unpadded_slices.append(torch.stack(unpadded_slice))\n",
    "\n",
    "    return torch.stack(unpadded_slices).permute(0,2,3,1)\n",
    "\n",
    "def logit_to_binary_mask(tensor, threshold=0.5):\n",
    "    # Transform a tensor of logits into a binary mask according to the specified probability threshold.\n",
    "    mask_tensor = torch.sigmoid(tensor)\n",
    "\n",
    "    return (mask_tensor >= threshold).float()\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filepath):\n",
    "    # Saves a checkpoint of a PyTorch model.\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "\n",
    "def save_array_to_nifti1(array, original_img, destination_path, output_name):\n",
    "    # Transform the array to a nifti image which requires the affine of the original image.\n",
    "    processed_img = nib.Nifti1Image(array, original_img.affine)\n",
    "    nib.save(processed_img, os.path.join(destination_path, output_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76050939-e922-464c-ae1d-addada7e527d",
   "metadata": {},
   "source": [
    "## Use segmentation models 3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c72051-e9ef-493e-92b3-3ed2ef397e56",
   "metadata": {},
   "source": [
    "### Dataset setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83bebbf3-5d8d-4890-a460-10ea9eafe3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicImageDataset3D(torch.utils.data.Dataset):\n",
    "    # PyTorch class used to read 3D image data and structure it into a tensor of double-channeled images and a tensor of binary mask labels.\n",
    "    def __init__(self, img_dir, label_dir, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.img_path_list = [[os.path.join(path, img) for img in files if img.endswith(\".nii.gz\")]\n",
    "                         for path, dirs, files in os.walk(self.img_dir) if path != self.img_dir]\n",
    "        self.label_path_list = [mask for mask in os.listdir(self.label_dir) if mask.endswith(\".nii.gz\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_path_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.array([nib.load(img[idx]).get_fdata() for img in self.img_path_list])\n",
    "        label = nib.load(os.path.join(self.label_dir, self.label_path_list[idx])).get_fdata()\n",
    "\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f34b7dfc-8c86-4bca-ae73-f91e5838f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Train_dataset\\\\\"\n",
    "val_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_dataset\"\n",
    "train_label_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Train_labels\\\\MASK_Train\"\n",
    "val_label_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_labels\\\\MASK_Val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f004094-a1a6-49ee-817c-b6984a10a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadToMultipleofN3D(object):\n",
    "    # A transform called by the dataset class, used to pad a 3D image to the closest multiple of a specified integer N (referred as multiple_n hereafter).\n",
    "    def __init__(self, multiple_n):\n",
    "        self.multiple_n = multiple_n\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # Take the last 3 dimensions as height, width and depth. Allows the class to be generalized to images with 1 or multiple channels.\n",
    "        height, width, depth = img.shape[-3:]\n",
    "        pad_height = (self.multiple_n - height % self.multiple_n) % self.multiple_n\n",
    "        pad_width = (self.multiple_n - width % self.multiple_n) % self.multiple_n\n",
    "        pad_depth = (self.multiple_n - depth % self.multiple_n) % self.multiple_n\n",
    "\n",
    "        padding = (0, pad_depth, 0, pad_height, 0, pad_width)\n",
    "\n",
    "        padded_img = torch.nn.functional.pad(img, padding, mode=\"constant\", value=0)\n",
    "        return padded_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09125ba1-faa5-4e52-91ec-45358b345c9d",
   "metadata": {},
   "source": [
    "### Use the BasicImageDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e21ece5e-c824-4fd9-898a-815927f2aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BasicImageDataset3D(train_dir, train_label_dir, transform=PadToMultipleofN3D(32), target_transform=PadToMultipleofN3D(32))\n",
    "val_dataset = BasicImageDataset3D(val_dir, val_label_dir, transform=PadToMultipleofN3D(32), target_transform=PadToMultipleofN3D(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cfe2ffd6-e85e-4406-b55f-0c898601598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f20d5-1181-44bc-91c2-dd43cd2a574d",
   "metadata": {},
   "source": [
    "### Use the SubjectsDataset class from torchio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0567a42f-56c5-49ff-84f3-a6821ecdf035",
   "metadata": {},
   "source": [
    "Create a mask that labels empty space (0), brain (1) and thrombus (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfe9e0e0-5ee9-4620-8b7c-f4ecfd5a49be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subjectsdataset (swi_dir, tof_dir, labels_dir, **kwargs):\n",
    "    subjects_list = []\n",
    "    swi_list = os.listdir(swi_dir)\n",
    "    tof_list = os.listdir(tof_dir)\n",
    "    labels_list = os.listdir(labels_dir)\n",
    "    \n",
    "    if len(swi_list) != len(tof_list) != len(labels_list):\n",
    "        print(\"Mismatch in sample numbers\")\n",
    "    \n",
    "    for swi_file, tof_file, label_file in zip(swi_list, tof_list, labels_list):\n",
    "        subject = tio.Subject(\n",
    "            swi_image=tio.ScalarImage(os.path.join(swi_dir, swi_file)),\n",
    "            tof_image=tio.ScalarImage(os.path.join(tof_dir, tof_file)),\n",
    "            label = tio.LabelMap(os.path.join(labels_dir, label_file)),\n",
    "            subject_number = \"_\".join(swi_file.split(\"_\")[:2])\n",
    "        )\n",
    "        subjects_list.append(subject)\n",
    "    \n",
    "    return tio.SubjectsDataset(subjects_list, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48623be8-971b-4b85-b794-76cc963d23d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "swi_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Torchio_train\\\\SWI_Train\"\n",
    "tof_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Torchio_train\\\\TOF3D_Train\"\n",
    "labels_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Torchio_labels\\\\MASK_Train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "913d27eb-e3b9-436c-bb91-6501c7e302f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tio_dataset = load_subjectsdataset(swi_dir, tof_dir, labels_dir, transform=tio.transforms.EnsureShapeMultiple(32, method=\"pad\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb5879d8-0653-4773-82a2-c72ba19bac3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelMap(shape: (1, 768, 768, 96); spacing: (0.30, 0.30, 1.50); orientation: RAS+; dtype: torch.IntTensor; memory: 216.0 MiB)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tio_dataset[1].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2859ab6-e8e9-40c2-9676-b3de7a338280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScalarImage(shape: (1, 768, 768, 90); spacing: (0.30, 0.30, 1.50); orientation: RAS+; dtype: torch.FloatTensor; memory: 202.5 MiB)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tio_dataset[1].swi_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63f7e43-2794-4de5-97ee-940fc19c2a7e",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f931dec7-85a1-4e05-b9df-07a8896f28c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp3d.UnetPlusPlus(encoder_name=\"resnext50_32x4d\", encoder_weights=\"imagenet\", in_channels=2, classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6626034-0625-4563-bb86-3297f30c5304",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss = smp3d.losses.DiceLoss(\"binary\", from_logits=False, smooth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8e86064-3999-4ee0-94c5-82a77c0054b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fc27b33f-5880-464d-88f7-f0824e9e9d39",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m images, gt_masks \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), gt_masks\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 13\u001b[0m predicted_mask \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m dice_loss(predicted_mask, gt_masks)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m## Average the predictions in each channel\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\segmentation_models_pytorch_3d\\base\\model.py:46\u001b[0m, in \u001b[0;36mSegmentationModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input_shape(x)\n\u001b[0;32m     45\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m---> 46\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmentation_head(decoder_output)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification_head \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\segmentation_models_pytorch_3d\\decoders\\unetplusplus\\decoder.py:129\u001b[0m, in \u001b[0;36mUnetPlusPlusDecoder.forward\u001b[1;34m(self, *features)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m depth_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m-\u001b[39m layer_idx):\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layer_idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 129\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdepth_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdepth_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdepth_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdepth_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m         dense_x[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdepth_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdepth_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\segmentation_models_pytorch_3d\\decoders\\unetplusplus\\decoder.py:38\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[1;34m(self, x, skip)\u001b[0m\n\u001b[0;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(x, scale_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention1(x)\n\u001b[0;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, gt_masks in train_dataloader:\n",
    "        images, gt_masks = images.to(device), gt_masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predicted_mask = model(images)\n",
    "        loss = dice_loss(predicted_mask, gt_masks)\n",
    "        ## Average the predictions in each channel\n",
    "        predicted_mask = predicted_mask.mean(dim=1)\n",
    "        # Transform prediction logits to probabilities\n",
    "        predicted_mask = torch.sigmoid(predicted_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, gt_masks in val_dataloader:\n",
    "            images, gt_masks = images.to(device), gt_masks.to(device)\n",
    "            \n",
    "            predicted_mask = model(images)\n",
    "            ## Average the predictions in each channel\n",
    "            predicted_mask = predicted_mask.mean(dim=1)\n",
    "            # Transform prediction logits to probabilities\n",
    "            predicted_mask = torch.sigmoid(predicted_mask)\n",
    "            loss = dice_loss(predicted_mask, gt_masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433444e5-e469-409c-8e18-294e6975eac1",
   "metadata": {},
   "source": [
    "## Use pytorch segmentation models slice-by-slice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cac5af-f0df-43ad-a5c2-ffba7c2d4d1a",
   "metadata": {},
   "source": [
    "### Dataset setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fd1e31f1-3511-451a-b074-0f70f60078ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicImageDataset2D(torch.utils.data.Dataset):\n",
    "    # PyTorch class used to read 3D image data and structure it into a tensor of double-channeled 2D slices and a tensor of binary mask labels.\n",
    "    def __init__(self, img_dir, label_dir, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.img_path_list = [[os.path.join(path, img) for img in files if img.endswith(\".nii.gz\")]\n",
    "                         for path, dirs, files in os.walk(self.img_dir) if path != self.img_dir]\n",
    "        self.label_path_list = [mask for mask in os.listdir(self.label_dir) if mask.endswith(\".nii.gz\")]\n",
    "        ## Assume all images have same dimensions by retrieving the dimensions of first image only.\n",
    "        self.img_depth = nib.load(os.path.join(self.label_dir, self.label_path_list[0])).shape[2] \n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.label_path_list) * self.img_depth)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // self.img_depth\n",
    "        slice_idx = idx % self.img_depth\n",
    "        img_slice = np.array([nib.load(img[img_idx]).get_fdata()[:,:,slice_idx] for img in self.img_path_list])\n",
    "        label_slice = nib.load(os.path.join(self.label_dir, self.label_path_list[img_idx])).get_fdata()[:,:,slice_idx]\n",
    "\n",
    "        img_slice = torch.tensor(img_slice, dtype=torch.float32)\n",
    "        label_slice = torch.tensor(label_slice, dtype=torch.float32)\n",
    "        if self.transform:\n",
    "            img_slice = self.transform(img_slice)\n",
    "        if self.target_transform:\n",
    "            label_slice = self.target_transform(label_slice)\n",
    "        return img_slice, label_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "15309a40-42fb-4ba9-afcf-1dbecaf09c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Train_dataset\\\\\"\n",
    "val_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_dataset\"\n",
    "train_label_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Train_labels\\\\MASK_Train\"\n",
    "val_label_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_labels\\\\MASK_Val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f45c95a1-c54a-451e-aee0-9b40a839999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadToMultipleofN2D(object):\n",
    "    # A transform called by the dataset class, used to pad a 2D image to the closest multiple of a specified integer N (referred as multiple_n hereafter).\n",
    "    def __init__(self, multiple_n):\n",
    "        self.multiple_n = multiple_n\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # Take the last 2 dimensions as height and width. Allows the class to be generalized to images with 1 or multiple channels.\n",
    "        height, width = img.shape[-2:]\n",
    "        pad_height = (self.multiple_n - height % self.multiple_n) % self.multiple_n\n",
    "        pad_width = (self.multiple_n - width % self.multiple_n) % self.multiple_n\n",
    "\n",
    "        padding = (0, pad_height, 0, pad_width)\n",
    "\n",
    "        padded_img = torch.nn.functional.pad(img, padding, mode=\"constant\", value=0)\n",
    "        return padded_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d402ace6-fd3d-4ee6-85d0-0608c7f2a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BasicImageDataset2D(train_dir, train_label_dir, transform=PadToMultipleofN2D(32), target_transform=PadToMultipleofN2D(32))\n",
    "val_dataset = BasicImageDataset2D(val_dir, val_label_dir, transform=PadToMultipleofN2D(32), target_transform=PadToMultipleofN2D(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2c8494a3-4714-433e-a1b4-a967e1e2e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6adae1a-8c1e-45d0-b2c2-300514a1b0ed",
   "metadata": {},
   "source": [
    "### Training Loop Slice-by-slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "4ffde041-bf34-4a0b-a3dc-2c7795e7c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.UnetPlusPlus(encoder_name=\"resnext50_32x4d\", encoder_weights=\"imagenet\", in_channels=2, classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "ff608029-926c-481c-bf8b-7481b538277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss = smp.losses.DiceLoss(\"binary\", from_logits=False, smooth=1)\n",
    "focal_loss = smp.losses.FocalLoss(\"binary\", alpha=0.75, gamma=2, normalized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "74ee2cb0-ad34-4c55-99de-975eb651a1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "cac1a2eb-f1c5-435e-9bb4-e29debc30efe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 0.0000\n",
      "Epoch 1/20, Validation Loss: 0.0000\n",
      "Epoch 2/20, Training Loss: 0.0000\n",
      "Epoch 2/20, Validation Loss: 0.0000\n",
      "Epoch 3/20, Training Loss: 0.0000\n",
      "Epoch 3/20, Validation Loss: 0.0000\n",
      "Epoch 4/20, Training Loss: 0.0000\n",
      "Epoch 4/20, Validation Loss: 0.0000\n",
      "Epoch 5/20, Training Loss: 0.0000\n",
      "Epoch 5/20, Validation Loss: 0.0000\n",
      "Epoch 6/20, Training Loss: 0.0000\n",
      "Epoch 6/20, Validation Loss: 0.0000\n",
      "Epoch 7/20, Training Loss: 0.0000\n",
      "Epoch 7/20, Validation Loss: 0.0000\n",
      "Epoch 8/20, Training Loss: 0.0000\n",
      "Epoch 8/20, Validation Loss: 0.0000\n",
      "Epoch 9/20, Training Loss: 0.0000\n",
      "Epoch 9/20, Validation Loss: 0.0000\n",
      "Epoch 10/20, Training Loss: 0.0000\n",
      "Epoch 10/20, Validation Loss: 0.0000\n",
      "Epoch 11/20, Training Loss: 0.0000\n",
      "Epoch 11/20, Validation Loss: 0.0000\n",
      "Epoch 12/20, Training Loss: 0.0000\n",
      "Epoch 12/20, Validation Loss: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[222], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      9\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_masks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_masks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_masks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[165], line 20\u001b[0m, in \u001b[0;36mBasicImageDataset2D.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     18\u001b[0m slice_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_depth\n\u001b[0;32m     19\u001b[0m img_slice \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([nib\u001b[38;5;241m.\u001b[39mload(img[img_idx])\u001b[38;5;241m.\u001b[39mget_fdata()[:,:,slice_idx] \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_path_list])\n\u001b[1;32m---> 20\u001b[0m label_slice \u001b[38;5;241m=\u001b[39m \u001b[43mnib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_path_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_fdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:,:,slice_idx]\n\u001b[0;32m     22\u001b[0m img_slice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(img_slice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     23\u001b[0m label_slice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(label_slice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nibabel\\dataobj_images.py:373\u001b[0m, in \u001b[0;36mDataobjImage.get_fdata\u001b[1;34m(self, caching, dtype)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fdata_cache\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# Always return requested data type\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# For array proxies, will attempt to confine data array to dtype\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# during scaling\u001b[39;00m\n\u001b[1;32m--> 373\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataobj, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m caching \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfill\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fdata_cache \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nibabel\\arrayproxy.py:457\u001b[0m, in \u001b[0;36mArrayProxy.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    437\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read data from file and apply scaling, casting to ``dtype``\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \n\u001b[0;32m    439\u001b[0m \u001b[38;5;124;03m    If ``dtype`` is unspecified, the dtype of the returned array is the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;124;03m        Scaled image data with type `dtype`.\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 457\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_scaled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslicer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m         arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nibabel\\arrayproxy.py:426\u001b[0m, in \u001b[0;36mArrayProxy._get_scaled\u001b[1;34m(self, dtype, slicer)\u001b[0m\n\u001b[0;32m    424\u001b[0m scaled \u001b[38;5;241m=\u001b[39m apply_read_scaling(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_unscaled(slicer\u001b[38;5;241m=\u001b[39mslicer), scl_slope, scl_inter)\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 426\u001b[0m     scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpromote_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scaled\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, gt_masks in train_dataloader:\n",
    "        images, gt_masks = images.to(device), gt_masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predicted_mask = model(images)\n",
    "        # Average the predictions in each channel\n",
    "        predicted_mask = predicted_mask.mean(dim=1)\n",
    "        # Transform prediction logits to probabilities\n",
    "        #predicted_mask = torch.sigmoid(predicted_mask)\n",
    "        loss = focal_loss(predicted_mask, gt_masks)\n",
    "        loss.backward()\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                pass\n",
    "                #print(f\"{name} gradient: {param.grad.norm().item()}\")\n",
    "            else:\n",
    "                print(f\"{name} has no gradient\")\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, gt_masks in val_dataloader:\n",
    "            images, gt_masks = images.to(device), gt_masks.to(device)\n",
    "            \n",
    "            predicted_mask = model(images)\n",
    "            # Average the predictions in each channel\n",
    "            predicted_mask = predicted_mask.mean(dim=1)\n",
    "            # Transform prediction logits to probabilities\n",
    "            #predicted_mask = torch.sigmoid(predicted_mask)\n",
    "            loss = focal_loss(predicted_mask, gt_masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "    save_checkpoint(model, optimizer, epoch, loss, 'D:\\\\data_ETIS_781\\\\Training\\\\Checkpoints\\\\downsampled_1p5_2p2_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a3d5d1-8a9b-46bf-ba0b-a40014a7d8ec",
   "metadata": {},
   "source": [
    "## Load model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "88860136-9af7-4109-8441-7240f91535c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_prediction = smp.UnetPlusPlus(encoder_name=\"resnext50_32x4d\", encoder_weights=\"imagenet\", in_channels=2, classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "d6304d2f-4cb8-4eea-9d90-a50fc1206ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer_for_prediction = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "05bb09cc-90dc-4a06-ba88-90161cf6916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_prediction_checkpoint = torch.load('D:\\\\data_ETIS_781\\\\Training\\\\Checkpoints\\\\downsampled_1p5_2p2_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "24ef5435-17bc-4791-92aa-61bb462af246",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_prediction.load_state_dict(model_for_prediction_checkpoint[\"model_state_dict\"])\n",
    "optimizer_for_prediction.load_state_dict(model_for_prediction_checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "ac5185ba-212b-4a01-8807-ee56a4a2ad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_dataset\\\\\"\n",
    "test_label_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_labels\\\\MASK_Val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "554c9d05-434f-4d38-ae29-16face140bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = BasicImageDataset2D(test_dir, test_label_dir, transform=PadToMultipleofN2D(32), target_transform=PadToMultipleofN2D(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "c25ccc5e-6f6a-46d8-9f3d-588f9aa2ba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "659087c6-76ae-44ab-b88f-1f2a46def0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "all_predictions_mean = []\n",
    "all_predictions_first = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, test_labels in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        predicted_mask_test = model(images)\n",
    "\n",
    "        predicted_mask_test_mean = predicted_mask_test.mean(dim=1)\n",
    "        predicted_mask_test_first = predicted_mask_test[:,0,:,:]\n",
    "\n",
    "        all_predictions_mean.append(predicted_mask_test_mean.cpu())\n",
    "        all_predictions_first.append(predicted_mask_test_first.cpu())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "10e49175-721f-49e1-8a2a-6312f199e9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_width = 160\n",
    "padded_height = 160\n",
    "depth = 62\n",
    "test_sample_size = 181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "65641f79-3099-4fce-9534-cd7f3334a122",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_predictions_mean_images = torch.cat(all_predictions_mean, dim=0).permute(1,2,0).reshape(test_sample_size,padded_width,padded_height,depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c23bb580-8ef9-4928-ba1f-1ee6e8630ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_first_images = torch.cat(all_predictions_first, dim=0).permute(1,2,0).reshape(test_sample_size,padded_width,padded_height,depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "1bcf8667-32ae-474e-9f88-61e19f63cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_height = 154\n",
    "original_width = 154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "50cdb3c7-f7ea-4c84-ba4c-6f1874ca1d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_mean_images = remove_padding_from_tensor(all_predictions_mean_images, original_height, original_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "29d1a7c5-601a-480a-a883-1ce6fb7a3f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_first_images = remove_padding_from_tensor(all_predictions_first_images, original_height, original_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "a9aa353e-278d-4de0-980a-5af43999d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_filepath = \"D:\\\\data_ETIS_781\\\\Training\\\\Test_labels\\\\MASK_Test\\\\2018-104_01-10113-D0MR_S9_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation_Downsampled.nii.gz\"\n",
    "reference_image = nib.load(reference_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "211f26a0-64ed-48a2-91f1-8638eab774a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_labels\\\\MASK_Val\"\n",
    "\n",
    "all_predictions_mean_masks = np.array(logit_to_binary_mask(all_predictions_mean_images, threshold=0.1))\n",
    "\n",
    "for mask_array, mask_filename in zip(all_predictions_mean_masks, os.listdir(test_labels_dir)):\n",
    "    split_filename = mask_filename.split(\".\")\n",
    "    split_filename[0] = split_filename[0] + \"_MeanPrediction\"\n",
    "    final_filename = \".\".join(split_filename)\n",
    "    save_array_to_nifti1(mask_array, reference_image, \"D:\\\\data_ETIS_781\\\\Training\\\\Predictions\\\\Validation_Predictions\\\\Mean_Predictions\", final_filename)\n",
    "\n",
    "del(all_predictions_mean_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "8786ed31-e4f4-40f0-b2a1-a3a5acc97a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_labels\\\\MASK_Val\"\n",
    "\n",
    "all_predictions_first_masks = np.array(logit_to_binary_mask(all_predictions_first_images, threshold=0.1))\n",
    "\n",
    "for mask_array, mask_filename in zip(all_predictions_first_masks, os.listdir(test_labels_dir)):\n",
    "    split_filename = mask_filename.split(\".\")\n",
    "    split_filename[0] = split_filename[0] + \"_SWIPrediction\"\n",
    "    final_filename = \".\".join(split_filename)\n",
    "    save_array_to_nifti1(mask_array, reference_image, \"D:\\\\data_ETIS_781\\\\Training\\\\Predictions\\\\Validation_Predictions\\\\SWI_Predictions\", final_filename)\n",
    "\n",
    "del(all_predictions_first_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "679c80c0-a022-461f-8d2d-b3490aca1cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([181, 154, 154, 62])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions_mean_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "81c9ee0b-5aca-42b6-b104-11888636618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_mean_sum = torch.sum(logit_to_binary_mask(all_predictions_mean_images, threshold=0.3), dim=(1,2,3))\n",
    "predictions_first_sum = torch.sum(logit_to_binary_mask(all_predictions_first_images, threshold=0.3), dim=(1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "cdaa66f9-9dcb-4c4c-933b-0e9607619092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1587200"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "160*160*62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "ae0fb986-0c5a-4303-b8bf-5700e5539808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  4., 10.,  6., 12., 13.,  4.,\n",
       "         7., 22., 23., 16., 24., 15.,  7., 18.,  6., 23., 27., 27., 15.,  4.,\n",
       "         9.,  9.,  1.,  6.,  7.,  5.,  3.,  2.,  3.,  1.,  3.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  2.,  3.,  7.,  0.,  4.,  6., 10.,  5., 14., 19., 26.,\n",
       "         7., 14., 10., 18., 25., 26., 12.,  7.,  4.,  9., 10., 16., 12.,  8.,\n",
       "         7.,  5.,  4.,  0.,  5.,  5.,  4.,  5.,  3.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_mean_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "51214ac1-3cbe-4b04-a908-7f63aae49323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  2.,  1.,  0.,  1.,  5.,  6., 21., 11., 19., 15., 11.,\n",
       "        16., 36., 32., 26., 38., 28., 24., 28., 15., 38., 41., 41., 26., 14.,\n",
       "        14., 12.,  1.,  9., 15.,  6., 10.,  6.,  7.,  5.,  7.,  1.,  2.,  2.,\n",
       "         2.,  1.,  0.,  5.,  4.,  9.,  0.,  9.,  9., 15., 10., 18., 35., 26.,\n",
       "         9., 18., 19., 29., 39., 37., 22., 13.,  6.,  9., 15., 17., 13., 12.,\n",
       "        22., 12., 13.,  2.,  7.,  8.,  6.,  7.,  4.,  1.,  0.,  2.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_first_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "55865892-3294-493b-a4f4-9454a5da1292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  0.,  1.,  3.,  0.,  0.,  0.,  0.,  5.,  0.,  0.,  0.,  0.,  5.,\n",
       "         3.,  0.,  1., 11.,  5.,  0.,  0.,  4.,  1.,  0.,  0.,  0.,  7.,  0.,\n",
       "         2.,  2.,  1.,  0.,  6.,  0.,  0.,  0.,  0.,  0., 16.,  4.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  7.,  0.,  0.,  0.,  0.,  0.,  1.,  3.,\n",
       "        21.,  0.,  3.,  0.,  4.,  3.,  0.,  5.,  0.,  6.,  0.,  1.,  0.,  0.,\n",
       "         0., 21.,  2.,  1.,  0.,  0.,  0.,  0.,  8.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  3.,  2.,  0.,  2., 14., 16.,  6.,  2.,  0.,  0., 13., 14.,\n",
       "         0.,  1.,  0.,  5.,  1.,  2.,  4.,  0., 40.,  2., 26., 12.,  0., 19.,\n",
       "         1., 13., 13., 45., 56., 13., 12.,  8.,  0.,  1.,  7.,  3.,  4.,  0.,\n",
       "         0.,  0.,  3.,  0.,  0.,  4.,  3.,  0.,  0.,  0.,  0.,  0.,  4.,  0.,\n",
       "         2., 19.,  8.,  0.,  0.,  0., 13.,  0.,  6.,  3.,  0.,  0., 10.,  0.,\n",
       "         0.,  0.,  0.,  0., 19.,  0., 29.,  4.,  1.,  6.,  1.,  8., 13.,  0.,\n",
       "         4.,  0.,  0., 15.,  2.,  3.,  0.,  0., 26.,  1., 12., 16.,  0.])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(val_extracted_labels, dim=(1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "cc151b5d-3076-45fc-b512-1a45cf1d335b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-104_05-10382-D0MR_S301_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation_Downsampled_SWIPrediction.nii.gz'"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"D:\\\\data_ETIS_781\\\\Training\\\\Predictions\\\\Validation_Predictions\\\\SWI_Predictions\")[62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "5bbecdb1-6d41-4e58-ac5e-b7e5c07b0a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_extracted_labels = [val_dataset[i][1] for i in range(len(val_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "224073da-ca3b-4dc1-a200-349d3a9d6558",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_extracted_labels = torch.tensor(np.array(val_extracted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "b501d0f3-029c-44bb-a6de-3f03e0870d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_extracted_labels = val_extracted_labels.reshape(181,160,160,62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "0742c3cc-ae62-4b25-adbc-318c5588565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_histogram = torch.flatten(torch.sigmoid(all_predictions_mean_images))\n",
    "swi_histogram = torch.flatten(torch.sigmoid(all_predictions_first_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "891a535a-ccc4-4241-9ab8-6e9ef1372d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1000.0)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "plt.hist(mean_histogram.numpy(),bins=50)\n",
    "ax.set_ylim(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "862205e3-5570-4510-a4cf-3b443e10e362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1000.0)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "plt.hist(swi_histogram.numpy(),bins=50)\n",
    "ax.set_ylim(0,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14820ab-ee9a-4089-ba50-ed63458882b7",
   "metadata": {},
   "source": [
    "## Testing ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "bfa76693-a1fc-485a-a965-246210a11603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.BasicImageDataset2D at 0x29825ee8fd0>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fde9e9a-a8f5-4480-892a-78f50b25da4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
