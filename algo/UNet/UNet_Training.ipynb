{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "102557f9-7184-484b-bd0b-79795636e49b",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7759bce3-13cf-4cda-a6e8-1f01f86b305c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wijflo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchio as tio\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import scipy\n",
    "import segmentation_models_pytorch as smp\n",
    "import segmentation_models_pytorch_3d as smp3d\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b2901c-63ed-44ae-88bd-8bdbaacd7bb4",
   "metadata": {},
   "source": [
    "## Filter out problematic images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d8537031-08db-41d9-8c64-d1fb376c5583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['06-10664-D0MR']\n",
      "['09-10683-D0MR']\n",
      "['09-10890-D0MR']\n",
      "['16-10232-D0MR']\n",
      "['21-10049-D0MR']\n",
      "['21-10049-D0MR']\n"
     ]
    }
   ],
   "source": [
    "source_dir = \"D:\\\\THROMBMICS-ALARMS_20240531\\\\SWI\"\n",
    "problem_dir = \"D:\\\\THROMBMICS-ALARMS_20240531\\\\Problem_Images\\\\SWI\"\n",
    "prev = []\n",
    "for file in os.listdir(source_dir):\n",
    "    number = file.split(\"_\")[1:2]\n",
    "    # if file.split(\"_\")[-1] == \"ph.nii.gz\":\n",
    "    #     ph_q = True\n",
    "    # else:\n",
    "    #     ph_q = False\n",
    "    if (number == prev):\n",
    "        print(number)\n",
    "        #os.rename(os.path.join(source_dir, file), os.path.join(problem_dir, file))\n",
    "    prev = number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "987fa1f8-78da-41d5-a7a2-da18b8b95351",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = \"D:\\\\THROMBMICS-ALARMS_20240531\\\\TOF3D\"\n",
    "problem_dir = \"D:\\\\THROMBMICS-ALARMS_20240531\\\\Problem_Images\\\\TOF3D\"\n",
    "prev = []\n",
    "for file in os.listdir(source_dir):\n",
    "    number = file.split(\"_\")[1:2]\n",
    "    if \"_\".join(file.split(\"_\")[-2:]) == \"Eq_1.nii.gz\":\n",
    "        eq1_q = True\n",
    "    else:\n",
    "        eq1_q = False\n",
    "    if (number == prev) & (eq1_q):\n",
    "        os.rename(os.path.join(source_dir, file), os.path.join(problem_dir, file))\n",
    "    prev = number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc631a49-7746-496f-9fb5-2e925805c890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'16-10170-D0MR', '02-10871-D0MR', '07-10333-D0MR', '14-10034-D0MR', '21-10163-D0MR', '21-10135-D0MR', '18-10428-D0MR', '01-10221-D0MR', '14-10119-D0MR', '14-10239-D0MR', '06-10750-D0MR', '05-10410-D0MR', '30-10034-D0MR', '18-10183-D0MR', '14-10164-D0MR', '30-10085-D0MR', '06-10487-D0MR', '14-10269-D0MR', '14-10115-D0MR', '18-10542-D0MR', '18-10099-D0MR', '06-10516-D0MR', '09-10890-D0MR', '21-10158-D0MR', '02-10874-D0MR', '30-10091-D0MR', '16-10168-D0MR', '30-10092-D0MR', '30-10090-D0MR', '06-10778-D0MR', '14-10156-D0MR', '02-10555-D0MR', '17-10120-D0MR', '16-10025-D0MR', '14-10172-D0MR', '02-10878-D0MR', '14-10068-D0MR', '06-10769-D0MR', '02-10722-D0MR', '14-10153-D0MR', '14-10238-D0MR', '30-10082-D0MR', '30-10083-D0MR', '07-10335-D0MR', '14-10120-D0MR', '30-10076-D0MR', '18-10396-D0MR', '14-10123-D0MR', '18-10206-D0MR', '14-10173-D0MR', '14-10087-D0MR', '21-10049-D0MR', '30-10088-D0MR', '14-10166-D0MR', '04-10442-D0MR', '14-10125-D0MR', '30-10094-D0MR', '14-10243-D0MR', '09-10674-D0MR', '09-10683-D0MR', '09-10670-D0MR', '30-10068-D0MR', '02-10652-D0MR', '30-10089-D0MR', '02-10653-D0MR', '18-10315-D0MR', '14-10171-D0MR', '04-10209-D0MR', '05-10383-D0MR', '30-10084-D0MR', '18-10037-D0MR', '11-10071-D0MR', '16-10117-D0MR', '18-10150-D0MR', '14-10086-D0MR', '30-10070-D0MR'}\n"
     ]
    }
   ],
   "source": [
    "swi_dir = \"D:\\\\THROMBMICS-ALARMS_20240531\\\\SWI\"\n",
    "mask_dir = \"D:\\\\THROMBMICS-ALARMS_20240531\\\\MASK\"\n",
    "\n",
    "swi_numbers = [file.split(\"_\")[1:2][0] for file in os.listdir(swi_dir)]\n",
    "mask_numbers = [file.split(\"_\")[1:2][0] for file in os.listdir(mask_dir)]\n",
    "\n",
    "diff = set(mask_numbers) - set(swi_numbers)\n",
    "\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "473d4408-8c01-425e-80ba-9a1b0bb3f0c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2018-104_16-10170-D0MR_22_AX_T2_EG.nii.gz\n",
      "Processed 2018-104_02-10871-D0MR_6_Ax_T2_GRE.nii.gz\n",
      "Processed 2018-104_07-10333-D0MR_20_SWI_Images.nii.gz\n",
      "Processed 2018-104_14-10034-D0MR_5_Ax_T2_.nii.gz\n",
      "Processed 2018-104_21-10163-D0MR_401_cs_T2_FFE.nii.gz\n",
      "Processed 2018-104_21-10135-D0MR_5_Ax_T2_.nii.gz\n",
      "Processed 2018-104_18-10428-D0MR_11_T2_EG_AX.nii.gz\n",
      "Processed 2018-104_01-10221-D0MR_11_SWI_Images.nii.gz\n",
      "Processed 2018-104_14-10119-D0MR_16_T2_EG_AX.nii.gz\n",
      "Processed 2018-104_14-10239-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_06-10750-D0MR_8_AX_T2_EG_STD.nii.gz\n",
      "Processed 2018-104_05-10410-D0MR_12_t2_fl2d_tra_4mm_hemo_te_25.nii.gz\n",
      "Processed 2018-104_30-10034-D0MR_8_t2_fl2d_ax.nii.gz\n",
      "Processed 2018-104_18-10183-D0MR_12_T2_EG_AX.nii.gz\n",
      "Processed 2018-104_14-10164-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_30-10085-D0MR_11_SWI_Images.nii.gz\n",
      "Processed 2018-104_06-10487-D0MR_9_AX_T2_EG_STD.nii.gz\n",
      "Processed 2018-104_14-10269-D0MR_16_AX_T2_.nii.gz\n",
      "Processed 2018-104_14-10115-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_18-10542-D0MR_8_AXIAL_T2_EG.nii.gz\n",
      "Processed 2018-104_18-10099-D0MR_5_Ax_T2_4mm.nii.gz\n",
      "Processed 2018-104_06-10516-D0MR_502_eAX_T2_.nii.gz\n",
      "Processed 2018-104_09-10890-D0MR_501_SWI_AVC.nii.gz\n",
      "Processed 2018-104_09-10890-D0MR_501_SWI_AVC_ph.nii.gz\n",
      "Processed 2018-104_21-10158-D0MR_401_cs_T2_FFE.nii.gz\n",
      "Processed 2018-104_02-10874-D0MR_6_Ax_eSWAN_2017.nii.gz\n",
      "Processed 2018-104_30-10091-D0MR_11_SWI_Images.nii.gz\n",
      "Processed 2018-104_16-10168-D0MR_15_SWI_Images.nii.gz\n",
      "Processed 2018-104_30-10092-D0MR_9_SWI_Images.nii.gz\n",
      "Processed 2018-104_30-10090-D0MR_11_SWI_Images.nii.gz\n",
      "Processed 2018-104_06-10778-D0MR_4_Ax_T2_GRE.nii.gz\n",
      "Processed 2018-104_14-10156-D0MR_6_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_02-10555-D0MR_5_3D_eSWAN_RAPIDE.nii.gz\n",
      "Processed 2018-104_17-10120-D0MR_6_Ax_T2_.nii.gz\n",
      "Processed 2018-104_16-10025-D0MR_12_T2_EG_AX.nii.gz\n",
      "Processed 2018-104_14-10172-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_02-10878-D0MR_5_Ax_T2_GRE.nii.gz\n",
      "Processed 2018-104_14-10068-D0MR_5_Ax_T2_.nii.gz\n",
      "Processed 2018-104_06-10769-D0MR_13_AX_T2_EG_STD.nii.gz\n",
      "Processed 2018-104_02-10722-D0MR_6_Ax_3D_SWAN+CARTO.nii.gz\n",
      "Processed 2018-104_14-10153-D0MR_5_3D_Ax_SWAN_4mm.nii.gz\n",
      "Processed 2018-104_14-10238-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_30-10082-D0MR_11_SWI_Images.nii.gz\n",
      "Processed 2018-104_30-10083-D0MR_13_SWI_Images.nii.gz\n",
      "Processed 2018-104_07-10335-D0MR_6_Ax_T2_GRE.nii.gz\n",
      "Processed 2018-104_14-10120-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_30-10076-D0MR_8_t2_fl2d_ax.nii.gz\n",
      "Processed 2018-104_18-10396-D0MR_12_T2_EG_AX.nii.gz\n",
      "Processed 2018-104_14-10123-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_18-10206-D0MR_11_T2_EG_AX.nii.gz\n",
      "Processed 2018-104_14-10173-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_14-10087-D0MR_5_Ax_SWAN_2.4MM.nii.gz\n",
      "Processed 2018-104_21-10049-D0MR_401_SWIp.nii.gz\n",
      "Processed 2018-104_21-10049-D0MR_401_SWIp_Eq_1.nii.gz\n",
      "Processed 2018-104_21-10049-D0MR_401_SWIp_ph.nii.gz\n",
      "Processed 2018-104_30-10088-D0MR_11_SWI_Images.nii.gz\n",
      "Processed 2018-104_14-10166-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_04-10442-D0MR_14_T2_EG_TRA.nii.gz\n",
      "Processed 2018-104_14-10125-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_30-10094-D0MR_4_Ax_T2_GRE.nii.gz\n",
      "Processed 2018-104_14-10243-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_09-10674-D0MR_5_Ax_T2_GRE.nii.gz\n",
      "Processed 2018-104_09-10683-D0MR_501_SWI_AVC.nii.gz\n",
      "Processed 2018-104_09-10683-D0MR_501_SWI_AVC_ph.nii.gz\n",
      "Processed 2018-104_09-10670-D0MR_502_AXIAL_SWI.nii.gz\n",
      "Processed 2018-104_30-10068-D0MR_8_t2_fl2d_ax.nii.gz\n",
      "Processed 2018-104_02-10652-D0MR_5_Ax_SWAN.nii.gz\n",
      "Processed 2018-104_30-10089-D0MR_15_SWI_Images.nii.gz\n",
      "Processed 2018-104_02-10653-D0MR_13_AX_T2_EG.nii.gz\n",
      "Processed 2018-104_18-10315-D0MR_11_T2_EG_AX.nii.gz\n",
      "Processed 2018-104_14-10171-D0MR_5_Ax_T2_GRE_rapide.nii.gz\n",
      "Processed 2018-104_04-10209-D0MR_5_Ax_T2_GRE.nii.gz\n",
      "Processed 2018-104_05-10383-D0MR_11_SWI_Images.nii.gz\n",
      "Processed 2018-104_30-10084-D0MR_11_SWI_Images.nii.gz\n",
      "Processed 2018-104_18-10037-D0MR_501_T2_FFE_CS.nii.gz\n",
      "Processed 2018-104_11-10071-D0MR_11_SWI_Images.nii.gz\n",
      "Processed 2018-104_16-10117-D0MR_13_T2_EG_AX.nii.gz\n",
      "Processed 2018-104_18-10150-D0MR_14_T2_EG_AX.nii.gz\n",
      "Processed 2018-104_14-10086-D0MR_6_3D_Ax_SWAN.nii.gz\n",
      "Processed 2018-104_30-10070-D0MR_8_t2_fl2d_ax.nii.gz\n"
     ]
    }
   ],
   "source": [
    "source_dir = \"E:\\\\Data_ETIS\\\\THROMBMICS-ALARMS_20240531\"\n",
    "target_dir = \"E:\\\\Data_ETIS\\\\Temp\"\n",
    "\n",
    "for number in list(diff):\n",
    "    for directory in glob.glob(os.path.join(source_dir, \"2018-104_\"+ number, \"T2star_*\")):\n",
    "        for nii_file in os.listdir(directory):\n",
    "            os.rename(os.path.join(directory, nii_file), os.path.join(target_dir, nii_file))\n",
    "            print(\"Processed \"+ nii_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b611b762-ea53-415a-af59-a47a1073ae11",
   "metadata": {},
   "source": [
    "## Separate Test Batch of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ffd9f8-2f8d-4b13-833a-02053ebea35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Train_labels\\\\MASK_Train\"\n",
    "mask_test_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Test_labels\\\\MASK_Test\"\n",
    "mask_val_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_labels\\\\MASK_Val\"\n",
    "swi_train_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Train_dataset\\\\SWI_Train\"\n",
    "swi_test_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Test_dataset\\\\SWI_Test\"\n",
    "swi_val_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_dataset\\\\SWI_Val\"\n",
    "tof_train_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Train_dataset\\\\TOF3D_Train\"\n",
    "tof_test_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Test_dataset\\\\TOF3D_Test\"\n",
    "tof_val_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_dataset\\\\TOF3D_Val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15cfda5e-2785-4836-ae83-889a9c3680ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_image_batch(mask_source, mask_destination, swi_source, swi_destination, tof_source, tof_destination, batch_size, seed_value=777):\n",
    "    # Separate three source folders, mask(labels), swi images, tof images into three other destination folders (e.g. validation or test),\n",
    "    # sending the specified number of images selected randomly.\n",
    "    random.seed(seed_value)\n",
    "    batch_indexes = random.sample(range(len(os.listdir(mask_source))), batch_size)\n",
    "\n",
    "    mask_file_list = [os.listdir(mask_source)[index] for index in batch_indexes]\n",
    "    for file in mask_file_list:\n",
    "        os.rename(os.path.join(mask_source, file), os.path.join(mask_destination, file))\n",
    "\n",
    "    swi_file_list = [os.listdir(swi_source)[index] for index in batch_indexes]\n",
    "    for file in swi_file_list:\n",
    "        os.rename(os.path.join(swi_source, file), os.path.join(swi_destination, file))\n",
    "\n",
    "    tof_file_list = [os.listdir(tof_source)[index] for index in batch_indexes]\n",
    "    for file in tof_file_list:\n",
    "        os.rename(os.path.join(tof_source, file), os.path.join(tof_destination, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0e1d3c-9c23-403f-8526-00efb1ca3d63",
   "metadata": {},
   "source": [
    "Separate test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "626d9491-7bfb-4bd3-9313-97dbe1b65efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_image_batch(mask_train_dir, mask_test_dir, swi_train_dir, swi_test_dir, tof_train_dir, tof_test_dir, 100, seed_value=777)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2501cdcb-4fc7-469c-bfe0-68f0386c28d6",
   "metadata": {},
   "source": [
    "Separate validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9ceb8cf-3ec5-480f-89c3-db6377789c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_image_batch(mask_train_dir, mask_val_dir, swi_train_dir, swi_val_dir, tof_train_dir, tof_val_dir, 181, seed_value=777)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192edc50-ce79-4d19-92b1-9c341d8a235c",
   "metadata": {},
   "source": [
    "Clear out the training folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a6f313a-6428-4483-930a-0dd5d31811e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_training_folders(mask_train_dir, mask_test_dir, mask_val_dir, swi_train_dir, swi_test_dir, swi_val_dir, tof_train_dir, tof_test_dir, tof_val_dir):\n",
    "    # Remove files from training, validation and test folders of masks, swi images and tof images.\n",
    "    folder_list = [mask_train_dir, mask_test_dir, mask_val_dir, swi_train_dir, swi_test_dir, swi_val_dir, tof_train_dir, tof_test_dir, tof_val_dir]\n",
    "    for folders in folder_list:\n",
    "        for file in os.listdir(folders):\n",
    "            os.remove(os.path.join(folders, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc04a0dc-cf1a-4b8c-af61-c0d3111a7a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_training_folders(mask_train_dir, mask_test_dir, mask_val_dir, swi_train_dir, swi_test_dir, swi_val_dir, tof_train_dir, tof_test_dir, tof_val_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f50055-f8bf-4c98-be7f-72b256862585",
   "metadata": {},
   "source": [
    "Fill training folders from processed images folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d97d0b10-438e-4df8-a04c-3ae0c5b33db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_training_folders(source_dir, mask_train_dir, swi_train_dir, tof_train_dir):\n",
    "    # Send images from a source folder containing MASK, SWI and TOF3D folders to the training folders.\n",
    "    for folders in os.listdir(source_dir):\n",
    "        if (folders.split(\"_\")[0] == \"MASK\") | (folders == \"MASK\"):\n",
    "            for files in os.listdir(os.path.join(source_dir, folders)):\n",
    "                os.rename(os.path.join(source_dir, folders, files), os.path.join(mask_train_dir,files))\n",
    "        elif folders.split(\"_\")[0] == \"SWI\":\n",
    "            for files in os.listdir(os.path.join(source_dir, folders)):\n",
    "                os.rename(os.path.join(source_dir, folders, files), os.path.join(swi_train_dir,files))\n",
    "        elif folders.split(\"_\")[0] == \"TOF3D\":\n",
    "            for files in os.listdir(os.path.join(source_dir, folders)):\n",
    "                os.rename(os.path.join(source_dir, folders, files), os.path.join(tof_train_dir,files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "245da8d3-712e-4e64-ae11-09e365713ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = \"D:\\\\data_ETIS_781\\\\Resized\"\n",
    "\n",
    "fill_training_folders(source_dir, mask_train_dir, swi_train_dir, tof_train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7944ba3-9b36-425b-a661-8cbbfd62772b",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "870e295a-7037-4fe4-9bfc-9e00b95faaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_padding_2D(slice_tensor, original_height, original_width):\n",
    "    # Remove padding from a 2D tensor, returning it to specified dimensions. The padding is removed from the right and from the bottom.\n",
    "    return slice_tensor[:original_height, :original_height]\n",
    "\n",
    "def remove_padding_from_tensor(tensor, original_height, original_width):\n",
    "    # Apply the padding removal function to the tensor and restack the slices to form 3D images.\n",
    "    unpadded_slices = []\n",
    "\n",
    "    for i in range(tensor.shape[0]):\n",
    "        unpadded_slice=[]\n",
    "        for j in range(tensor.shape[3]):\n",
    "            slice_tensor = tensor[i, :, :, j]\n",
    "            unpadded_slice.append(remove_padding_2D(slice_tensor, original_height, original_width))\n",
    "        unpadded_slices.append(torch.stack(unpadded_slice))\n",
    "\n",
    "    return torch.stack(unpadded_slices).permute(0,2,3,1)\n",
    "\n",
    "def logit_to_binary_mask(tensor, threshold=0.5):\n",
    "    # Transform a tensor of logits into a binary mask according to the specified probability threshold.\n",
    "    mask_tensor = torch.sigmoid(tensor)\n",
    "\n",
    "    return (mask_tensor >= threshold).float()\n",
    "\n",
    "def apply_processing_to_img_folder (processing_function, source_path, destination_path, modification_string, inclusion_string=\"\", **kwargs):\n",
    "    files = os.listdir(source_path)\n",
    "    \n",
    "    # Select files to process.\n",
    "    nifti_files = [file for file in files if (file.endswith('.nii.gz')) & (inclusion_string in file)]\n",
    "\n",
    "    for file in nifti_files:\n",
    "        file_path = os.path.join(source_path, file)\n",
    "        nii_img = nib.load(file_path)\n",
    "        nii_data = nii_img.get_fdata()\n",
    "        new_img_name = os.path.splitext(os.path.splitext(file)[0])[0] + \"_\" + modification_string + \".nii.gz\"\n",
    "\n",
    "        # Update kwargs dictionary to include file\n",
    "        kwargs[\"sample_filename\"] = file\n",
    "        \n",
    "        # Apply processing_function to the array, then save it as a nifti file.\n",
    "        save_array_to_nifti1(processing_function(nii_data, **kwargs), nii_img, destination_path, new_img_name)\n",
    "        print(\"Processed image \", file)\n",
    "\n",
    "def label_foreground_voxels (array, foreground_label=1.0, background_label=0.0, background_intensity=\"mode\"):\n",
    "    # Labels all voxels with different intensity as background.\n",
    "    \n",
    "    # If the default \"mode\" is selected as the background intensity, we use the most frequent voxel value as the background intensity value\n",
    "    if background_intensity == \"mode\":\n",
    "        background_value = scipy.stats.mode(array, axis=None).mode\n",
    "    # Otherwise the backgrond value must be supplied\n",
    "    else:\n",
    "        background_value = background_intensity\n",
    "\n",
    "    return np.where(array==background_value, background_label, foreground_label)\n",
    "\n",
    "def label_thrombus_and_foreground_voxels (array, sample_filename, label_directory, thrombus_label=2.0, **kwargs):\n",
    "    # Perform foreground labeling, then fetches thrombus label and labels thrombus on the foreground\n",
    "    labeled_array = label_foreground_voxels(array, **kwargs)\n",
    "\n",
    "    sample_number = \"_\".join(sample_filename.split(\"_\")[:2])\n",
    "    label_filename = glob.glob(os.path.join(label_directory, sample_number + \"*\"))[0]\n",
    "    label_mask = nib.load(label_filename).get_fdata().astype(\"bool\")\n",
    "\n",
    "    labeled_array[label_mask] = thrombus_label\n",
    "    \n",
    "    return labeled_array.astype(\"float32\")\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filepath):\n",
    "    # Saves a checkpoint of a PyTorch model.\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "\n",
    "def save_array_to_nifti1(array, original_img, destination_path, output_name):\n",
    "    # Transform the array to a nifti image which requires the affine of the original image.\n",
    "    processed_img = nib.Nifti1Image(array, original_img.affine)\n",
    "    nib.save(processed_img, os.path.join(destination_path, output_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c7e5433-ec4a-4c1b-9032-a96a229dec71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed image  2018-104_01-10087-D0MR_12_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10116-D0MR_7_AXIAL_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10117-D0MR_7_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10118-D0MR_12_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10124-D0MR_6_Ax_3D_SWAN_FAST_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10128-D0MR_7_Ax_3D_SWAN_FAST_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10151-D0MR_8_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10165-D0MR_13_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10172-D0MR_12_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10174-D0MR_10_AX_T2_EG__SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10175-D0MR_7_Ax_3D_SWAN+CARTO_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10177-D0MR_14_AX_T2_EG__SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10178-D0MR_7_AXIAL_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10180-D0MR_7_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10181-D0MR_14_AX_T2_EG__SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10183-D0MR_8_Ax_3D_SWAN+CARTO_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10184-D0MR_7_AXIAL_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10186-D0MR_12_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10191-D0MR_12_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10193-D0MR_6_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10196-D0MR_6_Ax_3D_SWAN_FAST_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10199-D0MR_10_AX_T2_EG__SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10200-D0MR_12_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10207-D0MR_7_Ax_3D_SWAN+CARTO_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10210-D0MR_5_T2_AX_EG_QUIET_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10217-D0MR_12_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10218-D0MR_12_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10220-D0MR_6_Ax_3D_SWAN_FAST_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10328-D0MR_13_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10356-D0MR_12_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10472-D0MR_7_AXIAL_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_01-10534-D0MR_12_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10241-D0MR_701_SWIp_50s_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10413-D0MR_6_3D_eSWAN_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10416-D0MR_12_T2_EG_TRA_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10420-D0MR_5_Ax_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10422-D0MR_6_3D_eSWAN_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10427-D0MR_701_SWIp_50s_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10428-D0MR_801_SWIp_50s_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10451-D0MR_6_3D_eSWAN_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10458-D0MR_5_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10503-D0MR_8_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10506-D0MR_701_SWIp_50s_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10509-D0MR_8_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10510-D0MR_6_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10513-D0MR_6_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10518-D0MR_6_3D_eSWAN_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10531-D0MR_13_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10552-D0MR_801_SWIp_50s_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10554-D0MR_801_SWIp_50s_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10556-D0MR_801_SWIp_50s_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10557-D0MR_801_SWIp_50s_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10562-D0MR_8_3D_eSWAN_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10563-D0MR_801_SWIp_50s_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10565-D0MR_7_3D_eSWAN_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10590-D0MR_801_SWIp_50s_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10591-D0MR_15001_AX_T2_EPI_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10592-D0MR_901_SWIp_50s_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10596-D0MR_13_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10600-D0MR_901_SWIp_50s_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10603-D0MR_5_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10606-D0MR_13_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10608-D0MR_6_3D_eSWAN_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10610-D0MR_801_SWIp_50s_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10617-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10618-D0MR_5_3D_eSWAN_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10620-D0MR_6_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10626-D0MR_701_SWIp_50s_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10628-D0MR_5_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10633-D0MR_8_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10634-D0MR_701_SWIp_50s_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10642-D0MR_13_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10645-D0MR_701_SWIp_50s_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10648-D0MR_5_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10649-D0MR_701_SWIp_50s_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10650-D0MR_6_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10652-D0MR_5_Ax_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10653-D0MR_13_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10656-D0MR_701_SWIp_50s_SkullStripped.nii.gz\n",
      "Processed image  2018-104_02-10659-D0MR_5_3D_eSWAN_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10173-D0MR_8_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10174-D0MR_13_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10175-D0MR_13_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10177-D0MR_12_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10178-D0MR_7_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10181-D0MR_8_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10182-D0MR_8_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10185-D0MR_13_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10187-D0MR_12_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10191-D0MR_9_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10205-D0MR_12_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10207-D0MR_10_T2_EG_HEMO_AXIAL_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10236-D0MR_12_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10239-D0MR_12_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10243-D0MR_8_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10245-D0MR_12_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10246-D0MR_12_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10247-D0MR_8_AX_T2_EG_HEMO_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10252-D0MR_9001_T2_EG_AX_HEMO_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10253-D0MR_13_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10257-D0MR_12_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10259-D0MR_13_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10260-D0MR_5_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10264-D0MR_7_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10266-D0MR_13_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10270-D0MR_5_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10272-D0MR_5_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10277-D0MR_7_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10285-D0MR_13_T2_EG_TRA_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10286-D0MR_13_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10294-D0MR_801_AX_T2_FFE_rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10295-D0MR_13_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10299-D0MR_13_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10300-D0MR_8_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10301-D0MR_13_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10305-D0MR_13_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10308-D0MR_301029_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10309-D0MR_10_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10310-D0MR_6_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10313-D0MR_13_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10315-D0MR_6_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10316-D0MR_13_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10322-D0MR_5_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10327-D0MR_8001_AX_T2_EG_HEMO_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10330-D0MR_13_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10332-D0MR_13_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10335-D0MR_7_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10338-D0MR_13_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10341-D0MR_13_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10342-D0MR_13_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10343-D0MR_9_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10348-D0MR_13_T2_EG_TRA_256_NEW_SkullStripped.nii.gz\n",
      "Processed image  2018-104_04-10442-D0MR_14_T2_EG_TRA_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10288-D0MR_6_Ax_3D_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10289-D0MR_401_3D_SWI_EPI_0_SkullStripped.7x2.0.nii.gz\n",
      "Processed image  2018-104_05-10293-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10294-D0MR_6_Ax_3D_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10296-D0MR_6_Ax_3D_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10297-D0MR_301_T2W_FFE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10308-D0MR_6_Ax_3D_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10310-D0MR_13_t2_fl2d_tra_4mm_hemo_te_25_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10319-D0MR_6_Ax_3D_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10327-D0MR_1001_T2W_FFE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10330-D0MR_10_AX_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10331-D0MR_401_3D_SWI_EPI_0_SkullStripped.7x2.0.nii.gz\n",
      "Processed image  2018-104_05-10332-D0MR_7_Ax_3D_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10339-D0MR_12_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10341-D0MR_6_Ax_3D_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10347-D0MR_6_Ax_3D_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10353-D0MR_6_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10354-D0MR_6_Ax_3D_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10355-D0MR_3_AX_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10356-D0MR_7_AX_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10358-D0MR_301_T2W_FFE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10363-D0MR_12_t2_fl2d_tra_4mm_hemo_te_25_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10364-D0MR_6_Ax_3D_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10366-D0MR_6_Ax_3D_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10367-D0MR_301_T2W_FFE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10379-D0MR_8_AX_T2_EG_HEMO_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10383-D0MR_11_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10386-D0MR_401_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10390-D0MR_301_T2W_FFE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10393-D0MR_8_AX_T2_EG_HEMO_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10394-D0MR_301_T2W_FFE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10395-D0MR_8_AXIAL_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10396-D0MR_501_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10399-D0MR_301_T2W_FFE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10402-D0MR_15_AXIAL_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10405-D0MR_401_3D_SWI_EPI_0_SkullStripped.7x2.0.nii.gz\n",
      "Processed image  2018-104_05-10406-D0MR_12_AX_T2_EG_HEMO_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10410-D0MR_12_t2_fl2d_tra_4mm_hemo_te_25_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10411-D0MR_8_Ax_3D_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10415-D0MR_6_Ax_3D_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10417-D0MR_6_Ax_3D_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10418-D0MR_6_Ax_3D_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10419-D0MR_301_T2W_FFE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10437-D0MR_6_Ax_3D_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10438-D0MR_6_Ax_3D_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10475-D0MR_401_T2W_FFE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10477-D0MR_301_T2W_FFE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10482-D0MR_12_AX_T2_EG_HEMO_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10483-D0MR_6_Ax_3D_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10485-D0MR_6_Ax_3D_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_05-10486-D0MR_6_Ax_3D_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10018-D0MR_601_T2_URG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10025-D0MR_11_AX_T2_EG_STD_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10030-D0MR_11_AX_T2_EG_STD_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10032-D0MR_13_AX_T2_EG_STD_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10034-D0MR_11_AX_T2_EG_STD_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10066-D0MR_7_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10068-D0MR_11_AX_T2_EG_STD_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10077-D0MR_501_T2_FFE_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10081-D0MR_8_AX_T2_EG_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10094-D0MR_11_AX_T2_EG_STD_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10118-D0MR_9_AX_T2_EG_STD_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10119-D0MR_8_AX_T2_EG_STD_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10129-D0MR_11_AX_T2_EG_STD_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10151-D0MR_301_T2_CS_FFE_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10161-D0MR_601_T2_URG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10166-D0MR_401_T2_cs_FFE_AX_-_40_s_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10167-D0MR_8_AX_T2_EG_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10168-D0MR_301_T2_FFE_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10174-D0MR_5_Ax_T2_GRE_5mm_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10195-D0MR_12_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10209-D0MR_12_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10242-D0MR_301_T2_FFE_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10252-D0MR_8_AX_T2_EG_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10254-D0MR_8_AX_T2_EG_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10256-D0MR_10_AX_T2_EG_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10260-D0MR_9_AX_T2_EG_STD_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10306-D0MR_5_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10324-D0MR_9_AX_T2_EG_STD_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10342-D0MR_301_T2_FFE_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10368-D0MR_8_AX_T2_EG_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10394-D0MR_11_AX_T2_EG_STD_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10420-D0MR_11_AX_T2_EG_STD_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10443-D0MR_11_T2_TRA_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10452-D0MR_12001_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10456-D0MR_401_T2_FFE_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10469-D0MR_401_T2_FFE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10487-D0MR_9_AX_T2_EG_STD_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10494-D0MR_5_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10535-D0MR_11_AX_T2_EG_STD_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10550-D0MR_9_AX_T2_EG_STD_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10576-D0MR_9_AX_T2_EG_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10616-D0MR_501_T2_FFE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10647-D0MR_502_eAX_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10656-D0MR_12_AX_T2_EG_STD_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10664-D0MR_601_T2_FFE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10750-D0MR_8_AX_T2_EG_STD_SkullStripped.nii.gz\n",
      "Processed image  2018-104_06-10778-D0MR_4_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_07-10005-D0MR_7_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_07-10011-D0MR_10001_AXIAL_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_07-10015-D0MR_6_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_07-10019-D0MR_7_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_07-10049-D0MR_5_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_07-10078-D0MR_6_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_07-10096-D0MR_7_T2_EG_AXIAL_SkullStripped.nii.gz\n",
      "Processed image  2018-104_07-10112-D0MR_8_AXIAL_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_07-10114-D0MR_7_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_07-10133-D0MR_7_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_07-10138-D0MR_7_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_07-10140-D0MR_7_Ax_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_07-10159-D0MR_5_Ax_SWAN_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_07-10184-D0MR_7_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_07-10191-D0MR_10_AXIAL_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_07-10333-D0MR_20_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_07-10335-D0MR_6_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10387-D0MR_8_T2_HEMO_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10388-D0MR_8_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10392-D0MR_301_cs_SWIp_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10397-D0MR_301_cs_SWIp_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10401-D0MR_11_T2_HEMO_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10411-D0MR_14001_T2_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10413-D0MR_10001_T2_AX_4mm_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10415-D0MR_12_T2_HEMO_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10418-D0MR_501_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10420-D0MR_501_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10427-D0MR_12_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10429-D0MR_501_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10433-D0MR_501_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10435-D0MR_10_AX_T2_EG_hemo_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10436-D0MR_501_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10437-D0MR_401_T2_FFE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10439-D0MR_501_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10440-D0MR_14_T2_EG_HEMO_AXIAL_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10441-D0MR_401_cs_SWIp_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10442-D0MR_501_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10449-D0MR_501_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10452-D0MR_501_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10456-D0MR_601_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10459-D0MR_13_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10463-D0MR_9_T2_HEMO_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10464-D0MR_501_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10466-D0MR_13_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10471-D0MR_601_T2_FFE_rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10474-D0MR_12_T2_HEMO_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10477-D0MR_401_cs_SWIp_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10482-D0MR_9_T2_HEMO_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10484-D0MR_501_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10488-D0MR_12_T2_HEMO_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10492-D0MR_501_cs_SWIp_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10501-D0MR_501_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10502-D0MR_501_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10504-D0MR_401_cs_SWIp_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10505-D0MR_501_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10506-D0MR_15_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10507-D0MR_14_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10509-D0MR_12_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10520-D0MR_9_T2_EG_HEMO_AXIAL_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10521-D0MR_501_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10525-D0MR_401_cs_SWIp_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10526-D0MR_601_T2_FFE_ax_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10537-D0MR_6_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10538-D0MR_501_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10545-D0MR_5_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10559-D0MR_16_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10564-D0MR_15_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10568-D0MR_301_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10569-D0MR_501_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10573-D0MR_10_T2_EG_HEMO_AXIAL_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10580-D0MR_15_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10582-D0MR_401_cs_SWIp_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10583-D0MR_16_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10587-D0MR_15_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10588-D0MR_501_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10592-D0MR_601_FFE_EPI_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10593-D0MR_9_T2_EG_HEMO_AXIAL_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10595-D0MR_8_T2_HEMO_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10596-D0MR_401_cs_SWIp_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10599-D0MR_11_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10604-D0MR_5_Ax_SWAN(+RAPIDE_)_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10615-D0MR_501_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10616-D0MR_10_T2_EG_HEMO_AXIAL_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10629-D0MR_7_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10631-D0MR_5_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10637-D0MR_10_T2_EG_HEMO_AXIAL_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10638-D0MR_401_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10643-D0MR_501_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10644-D0MR_501_SWIp_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10645-D0MR_12_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10651-D0MR_502_AXIAL_SWI_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10653-D0MR_7_T2_EG_HEMO_AXIAL_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10663-D0MR_502_AXIAL_SWI_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10670-D0MR_502_AXIAL_SWI_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10674-D0MR_5_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10712-D0MR_601_SWI_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10738-D0MR_501_SWI_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_09-10890-D0MR_501_SWI_AVC_SkullStripped.nii.gz\n",
      "Processed image  2018-104_11-10071-D0MR_11_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10001-D0MR_5_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10010-D0MR_5_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10011-D0MR_5_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10012-D0MR_15_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10018-D0MR_5_Ax_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10020-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10023-D0MR_16_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10028-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10030-D0MR_5_Ax_T2_GRE_rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10032-D0MR_6_Ax_T2_GRE_rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10034-D0MR_5_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10035-D0MR_15_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10038-D0MR_5_Ax_T2_GRE_rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10039-D0MR_15_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10040-D0MR_5_Ax_T2_GRE_rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10042-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10043-D0MR_5_Ax_T2_GRE_rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10052-D0MR_5_Ax_T2_GRE_rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10055-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10062-D0MR_5_Ax_T2_GRE_rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10063-D0MR_5_Ax_T2_GRE_rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10068-D0MR_5_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10086-D0MR_6_3D_Ax_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10092-D0MR_5_3D_Ax_SWAN_4mm_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10099-D0MR_5_Ax_T2_GRE_rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10111-D0MR_6_Ax_SWAN_2_SkullStripped.4MM.nii.gz\n",
      "Processed image  2018-104_14-10112-D0MR_15_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10115-D0MR_5_Ax_T2_GRE_rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10116-D0MR_11_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10119-D0MR_16_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10129-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10141-D0MR_5_Ax_T2_GRE_rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10142-D0MR_15_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10144-D0MR_5_Ax_T2_GRE_rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10152-D0MR_5_Ax_T2_GRE_rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10153-D0MR_5_3D_Ax_SWAN_4mm_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10166-D0MR_5_Ax_T2_GRE_rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10170-D0MR_4_Ax_T2_GRE_rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10171-D0MR_5_Ax_T2_GRE_rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10239-D0MR_5_Ax_T2_GRE_rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10243-D0MR_5_Ax_T2_GRE_rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_14-10269-D0MR_16_AX_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10003-D0MR_12_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10004-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10014-D0MR_9_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10015-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10021-D0MR_9_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10024-D0MR_8_Ax_eSWAN_2017_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10025-D0MR_12_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10026-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10028-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10031-D0MR_9_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10033-D0MR_5_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10041-D0MR_5_Ax_SWAN_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10049-D0MR_15_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10051-D0MR_15_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10052-D0MR_13_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10059-D0MR_9_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10063-D0MR_13_T2_GRE_TRA_2D_Rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10068-D0MR_6_Ax_eSWAN_2017_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10070-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10072-D0MR_9_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10075-D0MR_9_Ax_eSWAN_2017_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10079-D0MR_7_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10083-D0MR_12_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10088-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10089-D0MR_13_T2_GRE_TRA_2D_Rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10092-D0MR_10_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10098-D0MR_7_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10100-D0MR_17_T2_GRE_TRA_2D_Rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10104-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10106-D0MR_7_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10108-D0MR_7_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10111-D0MR_13_T2_GRE_TRA_2D_Rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10112-D0MR_8_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10113-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10115-D0MR_13_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10116-D0MR_13_T2_GRE_TRA_2D_Rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10122-D0MR_8_Ax_3D_SWAN_effet_tof_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10124-D0MR_5_Ax_SWAN_FAST_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10125-D0MR_7_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10130-D0MR_9_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10131-D0MR_13_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10132-D0MR_16_T2_EG_TRA_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10133-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10143-D0MR_12_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10144-D0MR_8_Ax_SWAN_FAST_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10145-D0MR_9_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10146-D0MR_10_T2_EG_AXIAL_HEMORRAGIQUE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10149-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10152-D0MR_13_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10153-D0MR_5_Ax_SWAN_FAST_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10154-D0MR_9_T2_GRE_TRA_2D_Rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10158-D0MR_7_Ax_SWAN_FAST_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10161-D0MR_9_T2_GRE_TRA_2D_Rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10167-D0MR_12_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10168-D0MR_15_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10171-D0MR_8_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10172-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10178-D0MR_10_T2_GRE_TRA_2D_Rapide_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10182-D0MR_13_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10183-D0MR_6_3D_SWAN_fast_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10188-D0MR_8_Ax_eSWAN_2017_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10189-D0MR_7_Ax_SWAN_FAST_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10190-D0MR_9_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10191-D0MR_9_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10193-D0MR_12_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10201-D0MR_13_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10207-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10212-D0MR_14_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10223-D0MR_13_T2_EG_AX_RAPIDE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_16-10226-D0MR_9_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_17-10022-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_17-10024-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_17-10120-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_17-10143-D0MR_9_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10001-D0MR_12_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10002-D0MR_501_T2_ETOILE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10004-D0MR_8_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10007-D0MR_12_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10010-D0MR_12_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10012-D0MR_12_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10014-D0MR_501_T2_ETOILE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10016-D0MR_14_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10018-D0MR_12_AX_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10019-D0MR_12_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10021-D0MR_6_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10030-D0MR_501_T2_ETOILE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10035-D0MR_11_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10037-D0MR_501_T2_FFE_CS_SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10038-D0MR_12_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10040-D0MR_12_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10041-D0MR_9_AXIAL_T2_EG_SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10043-D0MR_16_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10099-D0MR_5_Ax_T2_4mm_SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10150-D0MR_14_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10315-D0MR_11_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_18-10396-D0MR_12_T2_EG_AX_SkullStripped.nii.gz\n",
      "Processed image  2018-104_19-10031-D0MR_8_T2_EG_AXIAL_SkullStripped.nii.gz\n",
      "Processed image  2018-104_21-10049-D0MR_401_SWIp_SkullStripped.nii.gz\n",
      "Processed image  2018-104_21-10073-D0MR_8001_AX_T2_EG_3mm_SkullStripped.nii.gz\n",
      "Processed image  2018-104_21-10135-D0MR_5_Ax_T2__SkullStripped.nii.gz\n",
      "Processed image  2018-104_21-10163-D0MR_401_cs_T2_FFE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_22-10006-D0MR_13_T2_EG_TRA_OKFT_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10008-D0MR_11_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10015-D0MR_6_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10016-D0MR_8_t2_fl2d_ax_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10018-D0MR_8_t2_fl2d_ax_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10021-D0MR_11_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10023-D0MR_5_Ax_T2_GRE_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10026-D0MR_8_t2_fl2d_ax_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10034-D0MR_8_t2_fl2d_ax_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10040-D0MR_11_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10046-D0MR_8_t2_fl2d_ax_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10047-D0MR_13_t2_fl2d_ax_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10048-D0MR_8_t2_fl2d_ax_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10049-D0MR_8_t2_fl2d_ax_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10051-D0MR_8_t2_fl2d_ax_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10053-D0MR_8_t2_fl2d_ax_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10055-D0MR_8_t2_fl2d_ax_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10057-D0MR_8_t2_fl2d_ax_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10060-D0MR_11_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10068-D0MR_8_t2_fl2d_ax_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10070-D0MR_8_t2_fl2d_ax_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10082-D0MR_11_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10083-D0MR_13_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10084-D0MR_11_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10085-D0MR_11_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10088-D0MR_11_SWI_Images_SkullStripped.nii.gz\n",
      "Processed image  2018-104_30-10094-D0MR_4_Ax_T2_GRE_SkullStripped.nii.gz\n"
     ]
    }
   ],
   "source": [
    "apply_processing_to_img_folder (label_thrombus_and_foreground_voxels, \"D:\\\\data_ETIS_781\\\\Training\\\\Train_dataset\\\\SWI_Train\", \"D:\\\\data_ETIS_781\\\\Training\\\\Train_labels\\\\MASK_Foreground_Train\", \"ForegroundLabeled\", label_directory=\"D:\\\\data_ETIS_781\\\\Training\\\\Train_labels\\\\MASK_Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76050939-e922-464c-ae1d-addada7e527d",
   "metadata": {},
   "source": [
    "## Use segmentation models 3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c72051-e9ef-493e-92b3-3ed2ef397e56",
   "metadata": {},
   "source": [
    "### Dataset setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83bebbf3-5d8d-4890-a460-10ea9eafe3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicImageDataset3D(torch.utils.data.Dataset):\n",
    "    # PyTorch class used to read 3D image data and structure it into a tensor of double-channeled images and a tensor of binary mask labels.\n",
    "    def __init__(self, img_dir, label_dir, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.img_path_list = [[os.path.join(path, img) for img in files if img.endswith(\".nii.gz\")]\n",
    "                         for path, dirs, files in os.walk(self.img_dir) if path != self.img_dir]\n",
    "        self.label_path_list = [mask for mask in os.listdir(self.label_dir) if mask.endswith(\".nii.gz\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_path_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.array([nib.load(img[idx]).get_fdata() for img in self.img_path_list])\n",
    "        label = nib.load(os.path.join(self.label_dir, self.label_path_list[idx])).get_fdata()\n",
    "\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f34b7dfc-8c86-4bca-ae73-f91e5838f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Train_dataset\\\\\"\n",
    "val_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_dataset\"\n",
    "train_label_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Train_labels\\\\MASK_Train\"\n",
    "val_label_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_labels\\\\MASK_Val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f004094-a1a6-49ee-817c-b6984a10a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadToMultipleofN3D(object):\n",
    "    # A transform called by the dataset class, used to pad a 3D image to the closest multiple of a specified integer N (referred as multiple_n hereafter).\n",
    "    def __init__(self, multiple_n):\n",
    "        self.multiple_n = multiple_n\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # Take the last 3 dimensions as height, width and depth. Allows the class to be generalized to images with 1 or multiple channels.\n",
    "        height, width, depth = img.shape[-3:]\n",
    "        pad_height = (self.multiple_n - height % self.multiple_n) % self.multiple_n\n",
    "        pad_width = (self.multiple_n - width % self.multiple_n) % self.multiple_n\n",
    "        pad_depth = (self.multiple_n - depth % self.multiple_n) % self.multiple_n\n",
    "\n",
    "        padding = (0, pad_depth, 0, pad_height, 0, pad_width)\n",
    "\n",
    "        padded_img = torch.nn.functional.pad(img, padding, mode=\"constant\", value=0)\n",
    "        return padded_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09125ba1-faa5-4e52-91ec-45358b345c9d",
   "metadata": {},
   "source": [
    "### Use the BasicImageDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e21ece5e-c824-4fd9-898a-815927f2aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BasicImageDataset3D(train_dir, train_label_dir, transform=PadToMultipleofN3D(32), target_transform=PadToMultipleofN3D(32))\n",
    "val_dataset = BasicImageDataset3D(val_dir, val_label_dir, transform=PadToMultipleofN3D(32), target_transform=PadToMultipleofN3D(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cfe2ffd6-e85e-4406-b55f-0c898601598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f20d5-1181-44bc-91c2-dd43cd2a574d",
   "metadata": {},
   "source": [
    "### Use the SubjectsDataset class from torchio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0567a42f-56c5-49ff-84f3-a6821ecdf035",
   "metadata": {},
   "source": [
    "Create a mask that labels empty space (0), brain (1) and thrombus (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881ddd6-742b-4bbf-b2e7-cb77150501e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfe9e0e0-5ee9-4620-8b7c-f4ecfd5a49be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subjectsdataset (swi_dir, tof_dir, labels_dir, **kwargs):\n",
    "    subjects_list = []\n",
    "    swi_list = os.listdir(swi_dir)\n",
    "    tof_list = os.listdir(tof_dir)\n",
    "    labels_list = os.listdir(labels_dir)\n",
    "    \n",
    "    if len(swi_list) != len(tof_list) != len(labels_list):\n",
    "        print(\"Mismatch in sample numbers\")\n",
    "    \n",
    "    for swi_file, tof_file, label_file in zip(swi_list, tof_list, labels_list):\n",
    "        subject = tio.Subject(\n",
    "            swi_image=tio.ScalarImage(os.path.join(swi_dir, swi_file)),\n",
    "            tof_image=tio.ScalarImage(os.path.join(tof_dir, tof_file)),\n",
    "            label = tio.LabelMap(os.path.join(labels_dir, label_file)),\n",
    "            subject_number = \"_\".join(swi_file.split(\"_\")[:2])\n",
    "        )\n",
    "        subjects_list.append(subject)\n",
    "    \n",
    "    return tio.SubjectsDataset(subjects_list, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48623be8-971b-4b85-b794-76cc963d23d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "swi_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Torchio_train\\\\SWI_Train\"\n",
    "tof_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Torchio_train\\\\TOF3D_Train\"\n",
    "labels_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Torchio_labels\\\\MASK_Train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "913d27eb-e3b9-436c-bb91-6501c7e302f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tio_dataset = load_subjectsdataset(swi_dir, tof_dir, labels_dir, transform=tio.transforms.EnsureShapeMultiple(32, method=\"pad\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb5879d8-0653-4773-82a2-c72ba19bac3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelMap(shape: (1, 768, 768, 96); spacing: (0.30, 0.30, 1.50); orientation: RAS+; dtype: torch.IntTensor; memory: 216.0 MiB)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tio_dataset[1].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2859ab6-e8e9-40c2-9676-b3de7a338280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScalarImage(shape: (1, 768, 768, 90); spacing: (0.30, 0.30, 1.50); orientation: RAS+; dtype: torch.FloatTensor; memory: 202.5 MiB)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tio_dataset[1].swi_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63f7e43-2794-4de5-97ee-940fc19c2a7e",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f931dec7-85a1-4e05-b9df-07a8896f28c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp3d.UnetPlusPlus(encoder_name=\"resnext50_32x4d\", encoder_weights=\"imagenet\", in_channels=2, classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6626034-0625-4563-bb86-3297f30c5304",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss = smp3d.losses.DiceLoss(\"binary\", from_logits=False, smooth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8e86064-3999-4ee0-94c5-82a77c0054b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fc27b33f-5880-464d-88f7-f0824e9e9d39",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m images, gt_masks \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), gt_masks\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 13\u001b[0m predicted_mask \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m dice_loss(predicted_mask, gt_masks)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m## Average the predictions in each channel\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\segmentation_models_pytorch_3d\\base\\model.py:46\u001b[0m, in \u001b[0;36mSegmentationModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input_shape(x)\n\u001b[0;32m     45\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m---> 46\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmentation_head(decoder_output)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification_head \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\segmentation_models_pytorch_3d\\decoders\\unetplusplus\\decoder.py:129\u001b[0m, in \u001b[0;36mUnetPlusPlusDecoder.forward\u001b[1;34m(self, *features)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m depth_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m-\u001b[39m layer_idx):\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layer_idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 129\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdepth_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdepth_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdepth_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdepth_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m         dense_x[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdepth_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdepth_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\segmentation_models_pytorch_3d\\decoders\\unetplusplus\\decoder.py:38\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[1;34m(self, x, skip)\u001b[0m\n\u001b[0;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(x, scale_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention1(x)\n\u001b[0;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, gt_masks in train_dataloader:\n",
    "        images, gt_masks = images.to(device), gt_masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predicted_mask = model(images)\n",
    "        loss = dice_loss(predicted_mask, gt_masks)\n",
    "        ## Average the predictions in each channel\n",
    "        predicted_mask = predicted_mask.mean(dim=1)\n",
    "        # Transform prediction logits to probabilities\n",
    "        predicted_mask = torch.sigmoid(predicted_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, gt_masks in val_dataloader:\n",
    "            images, gt_masks = images.to(device), gt_masks.to(device)\n",
    "            \n",
    "            predicted_mask = model(images)\n",
    "            ## Average the predictions in each channel\n",
    "            predicted_mask = predicted_mask.mean(dim=1)\n",
    "            # Transform prediction logits to probabilities\n",
    "            predicted_mask = torch.sigmoid(predicted_mask)\n",
    "            loss = dice_loss(predicted_mask, gt_masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433444e5-e469-409c-8e18-294e6975eac1",
   "metadata": {},
   "source": [
    "## Use pytorch segmentation models slice-by-slice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cac5af-f0df-43ad-a5c2-ffba7c2d4d1a",
   "metadata": {},
   "source": [
    "### Dataset setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fd1e31f1-3511-451a-b074-0f70f60078ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicImageDataset2D(torch.utils.data.Dataset):\n",
    "    # PyTorch class used to read 3D image data and structure it into a tensor of double-channeled 2D slices and a tensor of binary mask labels.\n",
    "    def __init__(self, img_dir, label_dir, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.img_path_list = [[os.path.join(path, img) for img in files if img.endswith(\".nii.gz\")]\n",
    "                         for path, dirs, files in os.walk(self.img_dir) if path != self.img_dir]\n",
    "        self.label_path_list = [mask for mask in os.listdir(self.label_dir) if mask.endswith(\".nii.gz\")]\n",
    "        ## Assume all images have same dimensions by retrieving the dimensions of first image only.\n",
    "        self.img_depth = nib.load(os.path.join(self.label_dir, self.label_path_list[0])).shape[2] \n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.label_path_list) * self.img_depth)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // self.img_depth\n",
    "        slice_idx = idx % self.img_depth\n",
    "        img_slice = np.array([nib.load(img[img_idx]).get_fdata()[:,:,slice_idx] for img in self.img_path_list])\n",
    "        label_slice = nib.load(os.path.join(self.label_dir, self.label_path_list[img_idx])).get_fdata()[:,:,slice_idx]\n",
    "\n",
    "        img_slice = torch.tensor(img_slice, dtype=torch.float32)\n",
    "        label_slice = torch.tensor(label_slice, dtype=torch.float32)\n",
    "        if self.transform:\n",
    "            img_slice = self.transform(img_slice)\n",
    "        if self.target_transform:\n",
    "            label_slice = self.target_transform(label_slice)\n",
    "        return img_slice, label_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "15309a40-42fb-4ba9-afcf-1dbecaf09c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Train_dataset\\\\\"\n",
    "val_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_dataset\"\n",
    "train_label_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Train_labels\\\\MASK_Train\"\n",
    "val_label_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_labels\\\\MASK_Val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f45c95a1-c54a-451e-aee0-9b40a839999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadToMultipleofN2D(object):\n",
    "    # A transform called by the dataset class, used to pad a 2D image to the closest multiple of a specified integer N (referred as multiple_n hereafter).\n",
    "    def __init__(self, multiple_n):\n",
    "        self.multiple_n = multiple_n\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # Take the last 2 dimensions as height and width. Allows the class to be generalized to images with 1 or multiple channels.\n",
    "        height, width = img.shape[-2:]\n",
    "        pad_height = (self.multiple_n - height % self.multiple_n) % self.multiple_n\n",
    "        pad_width = (self.multiple_n - width % self.multiple_n) % self.multiple_n\n",
    "\n",
    "        padding = (0, pad_height, 0, pad_width)\n",
    "\n",
    "        padded_img = torch.nn.functional.pad(img, padding, mode=\"constant\", value=0)\n",
    "        return padded_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d402ace6-fd3d-4ee6-85d0-0608c7f2a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BasicImageDataset2D(train_dir, train_label_dir, transform=PadToMultipleofN2D(32), target_transform=PadToMultipleofN2D(32))\n",
    "val_dataset = BasicImageDataset2D(val_dir, val_label_dir, transform=PadToMultipleofN2D(32), target_transform=PadToMultipleofN2D(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2c8494a3-4714-433e-a1b4-a967e1e2e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6adae1a-8c1e-45d0-b2c2-300514a1b0ed",
   "metadata": {},
   "source": [
    "### Training Loop Slice-by-slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "4ffde041-bf34-4a0b-a3dc-2c7795e7c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.UnetPlusPlus(encoder_name=\"resnext50_32x4d\", encoder_weights=\"imagenet\", in_channels=2, classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "ff608029-926c-481c-bf8b-7481b538277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss = smp.losses.DiceLoss(\"binary\", from_logits=False, smooth=1)\n",
    "focal_loss = smp.losses.FocalLoss(\"binary\", alpha=0.75, gamma=2, normalized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "74ee2cb0-ad34-4c55-99de-975eb651a1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "cac1a2eb-f1c5-435e-9bb4-e29debc30efe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 0.0000\n",
      "Epoch 1/20, Validation Loss: 0.0000\n",
      "Epoch 2/20, Training Loss: 0.0000\n",
      "Epoch 2/20, Validation Loss: 0.0000\n",
      "Epoch 3/20, Training Loss: 0.0000\n",
      "Epoch 3/20, Validation Loss: 0.0000\n",
      "Epoch 4/20, Training Loss: 0.0000\n",
      "Epoch 4/20, Validation Loss: 0.0000\n",
      "Epoch 5/20, Training Loss: 0.0000\n",
      "Epoch 5/20, Validation Loss: 0.0000\n",
      "Epoch 6/20, Training Loss: 0.0000\n",
      "Epoch 6/20, Validation Loss: 0.0000\n",
      "Epoch 7/20, Training Loss: 0.0000\n",
      "Epoch 7/20, Validation Loss: 0.0000\n",
      "Epoch 8/20, Training Loss: 0.0000\n",
      "Epoch 8/20, Validation Loss: 0.0000\n",
      "Epoch 9/20, Training Loss: 0.0000\n",
      "Epoch 9/20, Validation Loss: 0.0000\n",
      "Epoch 10/20, Training Loss: 0.0000\n",
      "Epoch 10/20, Validation Loss: 0.0000\n",
      "Epoch 11/20, Training Loss: 0.0000\n",
      "Epoch 11/20, Validation Loss: 0.0000\n",
      "Epoch 12/20, Training Loss: 0.0000\n",
      "Epoch 12/20, Validation Loss: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[222], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      9\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_masks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_masks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_masks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[165], line 20\u001b[0m, in \u001b[0;36mBasicImageDataset2D.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     18\u001b[0m slice_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_depth\n\u001b[0;32m     19\u001b[0m img_slice \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([nib\u001b[38;5;241m.\u001b[39mload(img[img_idx])\u001b[38;5;241m.\u001b[39mget_fdata()[:,:,slice_idx] \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_path_list])\n\u001b[1;32m---> 20\u001b[0m label_slice \u001b[38;5;241m=\u001b[39m \u001b[43mnib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_path_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_fdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:,:,slice_idx]\n\u001b[0;32m     22\u001b[0m img_slice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(img_slice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     23\u001b[0m label_slice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(label_slice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nibabel\\dataobj_images.py:373\u001b[0m, in \u001b[0;36mDataobjImage.get_fdata\u001b[1;34m(self, caching, dtype)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fdata_cache\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# Always return requested data type\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# For array proxies, will attempt to confine data array to dtype\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# during scaling\u001b[39;00m\n\u001b[1;32m--> 373\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataobj, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m caching \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfill\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fdata_cache \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nibabel\\arrayproxy.py:457\u001b[0m, in \u001b[0;36mArrayProxy.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    437\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read data from file and apply scaling, casting to ``dtype``\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \n\u001b[0;32m    439\u001b[0m \u001b[38;5;124;03m    If ``dtype`` is unspecified, the dtype of the returned array is the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;124;03m        Scaled image data with type `dtype`.\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 457\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_scaled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslicer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m         arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nibabel\\arrayproxy.py:426\u001b[0m, in \u001b[0;36mArrayProxy._get_scaled\u001b[1;34m(self, dtype, slicer)\u001b[0m\n\u001b[0;32m    424\u001b[0m scaled \u001b[38;5;241m=\u001b[39m apply_read_scaling(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_unscaled(slicer\u001b[38;5;241m=\u001b[39mslicer), scl_slope, scl_inter)\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 426\u001b[0m     scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpromote_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scaled\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, gt_masks in train_dataloader:\n",
    "        images, gt_masks = images.to(device), gt_masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predicted_mask = model(images)\n",
    "        # Average the predictions in each channel\n",
    "        predicted_mask = predicted_mask.mean(dim=1)\n",
    "        # Transform prediction logits to probabilities\n",
    "        #predicted_mask = torch.sigmoid(predicted_mask)\n",
    "        loss = focal_loss(predicted_mask, gt_masks)\n",
    "        loss.backward()\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                pass\n",
    "                #print(f\"{name} gradient: {param.grad.norm().item()}\")\n",
    "            else:\n",
    "                print(f\"{name} has no gradient\")\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, gt_masks in val_dataloader:\n",
    "            images, gt_masks = images.to(device), gt_masks.to(device)\n",
    "            \n",
    "            predicted_mask = model(images)\n",
    "            # Average the predictions in each channel\n",
    "            predicted_mask = predicted_mask.mean(dim=1)\n",
    "            # Transform prediction logits to probabilities\n",
    "            #predicted_mask = torch.sigmoid(predicted_mask)\n",
    "            loss = focal_loss(predicted_mask, gt_masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "    save_checkpoint(model, optimizer, epoch, loss, 'D:\\\\data_ETIS_781\\\\Training\\\\Checkpoints\\\\downsampled_1p5_2p2_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a3d5d1-8a9b-46bf-ba0b-a40014a7d8ec",
   "metadata": {},
   "source": [
    "## Load model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "88860136-9af7-4109-8441-7240f91535c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_prediction = smp.UnetPlusPlus(encoder_name=\"resnext50_32x4d\", encoder_weights=\"imagenet\", in_channels=2, classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "d6304d2f-4cb8-4eea-9d90-a50fc1206ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer_for_prediction = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "05bb09cc-90dc-4a06-ba88-90161cf6916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_prediction_checkpoint = torch.load('D:\\\\data_ETIS_781\\\\Training\\\\Checkpoints\\\\downsampled_1p5_2p2_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "24ef5435-17bc-4791-92aa-61bb462af246",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_prediction.load_state_dict(model_for_prediction_checkpoint[\"model_state_dict\"])\n",
    "optimizer_for_prediction.load_state_dict(model_for_prediction_checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "ac5185ba-212b-4a01-8807-ee56a4a2ad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_dataset\\\\\"\n",
    "test_label_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_labels\\\\MASK_Val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "554c9d05-434f-4d38-ae29-16face140bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = BasicImageDataset2D(test_dir, test_label_dir, transform=PadToMultipleofN2D(32), target_transform=PadToMultipleofN2D(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "c25ccc5e-6f6a-46d8-9f3d-588f9aa2ba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "659087c6-76ae-44ab-b88f-1f2a46def0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "all_predictions_mean = []\n",
    "all_predictions_first = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, test_labels in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        predicted_mask_test = model(images)\n",
    "\n",
    "        predicted_mask_test_mean = predicted_mask_test.mean(dim=1)\n",
    "        predicted_mask_test_first = predicted_mask_test[:,0,:,:]\n",
    "\n",
    "        all_predictions_mean.append(predicted_mask_test_mean.cpu())\n",
    "        all_predictions_first.append(predicted_mask_test_first.cpu())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "10e49175-721f-49e1-8a2a-6312f199e9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_width = 160\n",
    "padded_height = 160\n",
    "depth = 62\n",
    "test_sample_size = 181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "65641f79-3099-4fce-9534-cd7f3334a122",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_predictions_mean_images = torch.cat(all_predictions_mean, dim=0).permute(1,2,0).reshape(test_sample_size,padded_width,padded_height,depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c23bb580-8ef9-4928-ba1f-1ee6e8630ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_first_images = torch.cat(all_predictions_first, dim=0).permute(1,2,0).reshape(test_sample_size,padded_width,padded_height,depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "1bcf8667-32ae-474e-9f88-61e19f63cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_height = 154\n",
    "original_width = 154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "50cdb3c7-f7ea-4c84-ba4c-6f1874ca1d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_mean_images = remove_padding_from_tensor(all_predictions_mean_images, original_height, original_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "29d1a7c5-601a-480a-a883-1ce6fb7a3f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_first_images = remove_padding_from_tensor(all_predictions_first_images, original_height, original_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "a9aa353e-278d-4de0-980a-5af43999d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_filepath = \"D:\\\\data_ETIS_781\\\\Training\\\\Test_labels\\\\MASK_Test\\\\2018-104_01-10113-D0MR_S9_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation_Downsampled.nii.gz\"\n",
    "reference_image = nib.load(reference_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "211f26a0-64ed-48a2-91f1-8638eab774a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_labels\\\\MASK_Val\"\n",
    "\n",
    "all_predictions_mean_masks = np.array(logit_to_binary_mask(all_predictions_mean_images, threshold=0.1))\n",
    "\n",
    "for mask_array, mask_filename in zip(all_predictions_mean_masks, os.listdir(test_labels_dir)):\n",
    "    split_filename = mask_filename.split(\".\")\n",
    "    split_filename[0] = split_filename[0] + \"_MeanPrediction\"\n",
    "    final_filename = \".\".join(split_filename)\n",
    "    save_array_to_nifti1(mask_array, reference_image, \"D:\\\\data_ETIS_781\\\\Training\\\\Predictions\\\\Validation_Predictions\\\\Mean_Predictions\", final_filename)\n",
    "\n",
    "del(all_predictions_mean_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "8786ed31-e4f4-40f0-b2a1-a3a5acc97a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Validation_labels\\\\MASK_Val\"\n",
    "\n",
    "all_predictions_first_masks = np.array(logit_to_binary_mask(all_predictions_first_images, threshold=0.1))\n",
    "\n",
    "for mask_array, mask_filename in zip(all_predictions_first_masks, os.listdir(test_labels_dir)):\n",
    "    split_filename = mask_filename.split(\".\")\n",
    "    split_filename[0] = split_filename[0] + \"_SWIPrediction\"\n",
    "    final_filename = \".\".join(split_filename)\n",
    "    save_array_to_nifti1(mask_array, reference_image, \"D:\\\\data_ETIS_781\\\\Training\\\\Predictions\\\\Validation_Predictions\\\\SWI_Predictions\", final_filename)\n",
    "\n",
    "del(all_predictions_first_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "679c80c0-a022-461f-8d2d-b3490aca1cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([181, 154, 154, 62])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions_mean_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "81c9ee0b-5aca-42b6-b104-11888636618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_mean_sum = torch.sum(logit_to_binary_mask(all_predictions_mean_images, threshold=0.3), dim=(1,2,3))\n",
    "predictions_first_sum = torch.sum(logit_to_binary_mask(all_predictions_first_images, threshold=0.3), dim=(1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "cdaa66f9-9dcb-4c4c-933b-0e9607619092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1587200"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "160*160*62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "ae0fb986-0c5a-4303-b8bf-5700e5539808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  4., 10.,  6., 12., 13.,  4.,\n",
       "         7., 22., 23., 16., 24., 15.,  7., 18.,  6., 23., 27., 27., 15.,  4.,\n",
       "         9.,  9.,  1.,  6.,  7.,  5.,  3.,  2.,  3.,  1.,  3.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  2.,  3.,  7.,  0.,  4.,  6., 10.,  5., 14., 19., 26.,\n",
       "         7., 14., 10., 18., 25., 26., 12.,  7.,  4.,  9., 10., 16., 12.,  8.,\n",
       "         7.,  5.,  4.,  0.,  5.,  5.,  4.,  5.,  3.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_mean_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "51214ac1-3cbe-4b04-a908-7f63aae49323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  2.,  1.,  0.,  1.,  5.,  6., 21., 11., 19., 15., 11.,\n",
       "        16., 36., 32., 26., 38., 28., 24., 28., 15., 38., 41., 41., 26., 14.,\n",
       "        14., 12.,  1.,  9., 15.,  6., 10.,  6.,  7.,  5.,  7.,  1.,  2.,  2.,\n",
       "         2.,  1.,  0.,  5.,  4.,  9.,  0.,  9.,  9., 15., 10., 18., 35., 26.,\n",
       "         9., 18., 19., 29., 39., 37., 22., 13.,  6.,  9., 15., 17., 13., 12.,\n",
       "        22., 12., 13.,  2.,  7.,  8.,  6.,  7.,  4.,  1.,  0.,  2.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_first_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "55865892-3294-493b-a4f4-9454a5da1292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  0.,  1.,  3.,  0.,  0.,  0.,  0.,  5.,  0.,  0.,  0.,  0.,  5.,\n",
       "         3.,  0.,  1., 11.,  5.,  0.,  0.,  4.,  1.,  0.,  0.,  0.,  7.,  0.,\n",
       "         2.,  2.,  1.,  0.,  6.,  0.,  0.,  0.,  0.,  0., 16.,  4.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  7.,  0.,  0.,  0.,  0.,  0.,  1.,  3.,\n",
       "        21.,  0.,  3.,  0.,  4.,  3.,  0.,  5.,  0.,  6.,  0.,  1.,  0.,  0.,\n",
       "         0., 21.,  2.,  1.,  0.,  0.,  0.,  0.,  8.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  3.,  2.,  0.,  2., 14., 16.,  6.,  2.,  0.,  0., 13., 14.,\n",
       "         0.,  1.,  0.,  5.,  1.,  2.,  4.,  0., 40.,  2., 26., 12.,  0., 19.,\n",
       "         1., 13., 13., 45., 56., 13., 12.,  8.,  0.,  1.,  7.,  3.,  4.,  0.,\n",
       "         0.,  0.,  3.,  0.,  0.,  4.,  3.,  0.,  0.,  0.,  0.,  0.,  4.,  0.,\n",
       "         2., 19.,  8.,  0.,  0.,  0., 13.,  0.,  6.,  3.,  0.,  0., 10.,  0.,\n",
       "         0.,  0.,  0.,  0., 19.,  0., 29.,  4.,  1.,  6.,  1.,  8., 13.,  0.,\n",
       "         4.,  0.,  0., 15.,  2.,  3.,  0.,  0., 26.,  1., 12., 16.,  0.])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(val_extracted_labels, dim=(1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "cc151b5d-3076-45fc-b512-1a45cf1d335b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-104_05-10382-D0MR_S301_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation_Downsampled_SWIPrediction.nii.gz'"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"D:\\\\data_ETIS_781\\\\Training\\\\Predictions\\\\Validation_Predictions\\\\SWI_Predictions\")[62]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14820ab-ee9a-4089-ba50-ed63458882b7",
   "metadata": {},
   "source": [
    "## Testing ground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5ce29-fbc4-48e3-9517-0b97ff978f15",
   "metadata": {},
   "source": [
    "### Get dimensions of label bounding cuboid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9292860d-dd9e-46a9-8b46-437d4ddba41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_cuboid_dimensions_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfa76693-a1fc-485a-a965-246210a11603",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask number 2018-104_01-10113-D0MR_S9_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 1\n",
      "Processed mask number 2\n",
      "Processed mask number 3\n",
      "Processed mask number 4\n",
      "Mask number 2018-104_01-10209-D0MR_S15_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 5\n",
      "Mask number 2018-104_01-10317-D0MR_S11_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 6\n",
      "Processed mask number 7\n",
      "Processed mask number 8\n",
      "Processed mask number 9\n",
      "Mask number 2018-104_02-10601-D0MR_S5_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 10\n",
      "Mask number 2018-104_02-10613-D0MR_S1001_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 11\n",
      "Mask number 2018-104_02-10637-D0MR_S5_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 12\n",
      "Processed mask number 13\n",
      "Mask number 2018-104_02-10651-D0MR_S5_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 14\n",
      "Processed mask number 15\n",
      "Processed mask number 16\n",
      "Mask number 2018-104_02-10878-D0MR_S5_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 17\n",
      "Mask number 2018-104_04-10166-D0MR_S8_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 18\n",
      "Mask number 2018-104_04-10168-D0MR_S8_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 19\n",
      "Mask number 2018-104_04-10241-D0MR_S12_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 20\n",
      "Mask number 2018-104_04-10284-D0MR_S13_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 21\n",
      "Mask number 2018-104_04-10290-D0MR_S13_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 22\n",
      "Mask number 2018-104_04-10302-D0MR_S13_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 23\n",
      "Mask number 2018-104_04-10317-D0MR_S6_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 24\n",
      "Mask number 2018-104_05-10295-D0MR_S16_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 25\n",
      "Processed mask number 26\n",
      "Mask number 2018-104_05-10349-D0MR_S5_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 27\n",
      "Processed mask number 28\n",
      "Processed mask number 29\n",
      "Processed mask number 30\n",
      "Processed mask number 31\n",
      "Processed mask number 32\n",
      "Processed mask number 33\n",
      "Processed mask number 34\n",
      "Processed mask number 35\n",
      "Mask number 2018-104_06-10263-D0MR_S8_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 36\n",
      "Processed mask number 37\n",
      "Mask number 2018-104_06-10516-D0MR_S502_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 38\n",
      "Mask number 2018-104_06-10552-D0MR_S8_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 39\n",
      "Mask number 2018-104_06-10610-D0MR_S5_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 40\n",
      "Processed mask number 41\n",
      "Processed mask number 42\n",
      "Processed mask number 43\n",
      "Mask number 2018-104_09-10412-D0MR_S5_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 44\n",
      "Processed mask number 45\n",
      "Processed mask number 46\n",
      "Mask number 2018-104_09-10432-D0MR_S15_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 47\n",
      "Processed mask number 48\n",
      "Processed mask number 49\n",
      "Mask number 2018-104_09-10467-D0MR_S13_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 50\n",
      "Processed mask number 51\n",
      "Mask number 2018-104_09-10522-D0MR_S7_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 52\n",
      "Processed mask number 53\n",
      "Processed mask number 54\n",
      "Processed mask number 55\n",
      "Mask number 2018-104_09-10648-D0MR_S601_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 56\n",
      "Processed mask number 57\n",
      "Processed mask number 58\n",
      "Processed mask number 59\n",
      "Mask number 2018-104_14-10075-D0MR_S5_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 60\n",
      "Mask number 2018-104_14-10120-D0MR_S5_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 61\n",
      "Mask number 2018-104_14-10123-D0MR_S5_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 62\n",
      "Processed mask number 63\n",
      "Mask number 2018-104_14-10162-D0MR_S6_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 64\n",
      "Processed mask number 65\n",
      "Processed mask number 66\n",
      "Mask number 2018-104_16-10006-D0MR_S6_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 67\n",
      "Processed mask number 68\n",
      "Processed mask number 69\n",
      "Mask number 2018-104_16-10037-D0MR_S13_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 70\n",
      "Processed mask number 71\n",
      "Processed mask number 72\n",
      "Processed mask number 73\n",
      "Processed mask number 74\n",
      "Mask number 2018-104_16-10118-D0MR_S14_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 75\n",
      "Processed mask number 76\n",
      "Processed mask number 77\n",
      "Processed mask number 78\n",
      "Mask number 2018-104_16-10174-D0MR_S6_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 79\n",
      "Mask number 2018-104_16-10185-D0MR_S10_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 80\n",
      "Processed mask number 81\n",
      "Processed mask number 82\n",
      "Processed mask number 83\n",
      "Processed mask number 84\n",
      "Processed mask number 85\n",
      "Mask number 2018-104_16-10231-D0MR_S3_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 86\n",
      "Mask number 2018-104_17-10085-D0MR_S5_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 87\n",
      "Processed mask number 88\n",
      "Processed mask number 89\n",
      "Processed mask number 90\n",
      "Mask number 2018-104_19-10040-D0MR_S8_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 91\n",
      "Mask number 2018-104_21-10158-D0MR_S401_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 92\n",
      "Processed mask number 93\n",
      "Processed mask number 94\n",
      "Mask number 2018-104_30-10024-D0MR_S8_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 95\n",
      "Processed mask number 96\n",
      "Processed mask number 97\n",
      "Processed mask number 98\n",
      "Processed mask number 99\n",
      "Mask number 2018-104_30-10092-D0MR_S9_202312051300_Resized_Tours-THROMBMICS-Clot-Segmentation.nii.gz has 0 size label\n",
      "Processed mask number 100\n"
     ]
    }
   ],
   "source": [
    "mask_dir = \"D:\\\\data_ETIS_781\\\\Training\\\\Test_labels\\\\MASK_Test\"\n",
    "mask_counter = 0\n",
    "\n",
    "for mask in os.listdir(mask_dir):\n",
    "    mask_counter += 1\n",
    "    mask_array = nib.load(os.path.join(mask_dir,mask)).get_fdata()\n",
    "    bounding_cuboid_dimensions = get_bounding_cuboid_dimensions(mask_array)\n",
    "    bounding_cuboid_dimensions_list.append(bounding_cuboid_dimensions)\n",
    "\n",
    "    if bounding_cuboid_dimensions == (0,0,0):\n",
    "        print(f\"Mask number {mask} has 0 size label\")\n",
    "    print(f\"Processed mask number {mask_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fde9e9a-a8f5-4480-892a-78f50b25da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_cuboid_dimensions (array):\n",
    "    labeled_indices = np.argwhere(array == 1.)\n",
    "\n",
    "    if labeled_indices.size == 0:\n",
    "        return (0,0,0)\n",
    "    \n",
    "    min_coords = labeled_indices.min(axis=0)\n",
    "    max_coords = labeled_indices.max(axis=0)\n",
    "\n",
    "    x_length = max_coords[0] - min_coords[0] + 1\n",
    "    y_length = max_coords[1] - min_coords[2] + 1\n",
    "    z_length = max_coords[2] - min_coords[2] + 1\n",
    "\n",
    "    return (x_length, y_length, z_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4567d7cc-db0f-4a8b-ba29-5b655c85a20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "781"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bounding_cuboid_dimensions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "045715dc-59a9-4899-9b10-f3e45119ebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for elem in bounding_cuboid_dimensions_list:\n",
    "    if elem == (0,0,0):\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea162470-059b-4656-a96c-2c58ab62262f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e6310b-cd73-429b-b97c-c57eea9889ec",
   "metadata": {},
   "source": [
    "### Test resizing on labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71009340-cae6-478b-b252-6abe6e536ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_product(*args):\n",
    "    product = 1\n",
    "    for element in args:\n",
    "        product *= element\n",
    "    return product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f65d92c3-c864-470b-ae80-4da53081e52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"G:\\\\Data_Booster\\\\data_ETIS_781\\\\SWI\"\n",
    "nifti_files = [file for file in os.listdir(source_path) if file.endswith('.nii.gz')]\n",
    "nii_img_shapes = []\n",
    "\n",
    "for file in nifti_files:\n",
    "        file_path = os.path.join(source_path, file)\n",
    "        nii_img = nib.load(file_path)\n",
    "        nii_img_shapes.append(nii_img.shape)\n",
    "\n",
    "index_of_max_product = max(range(len(nii_img_shapes)), key=lambda i: tuple_product(nii_img_shapes[i]))\n",
    "        \n",
    "reference_filename = nifti_files[index_of_max_product]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd1422c6-9832-4fb9-b157-a64daaaa2231",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_img = tio.ScalarImage(os.path.join(source_path,reference_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65b5df3c-59dd-44bd-8ee5-474e87eafde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_scalar_img = tio.ScalarImage(\"G:\\\\Data_Booster\\\\data_ETIS_781\\\\MASK\\\\2018-104_01-10117-D0MR_S7_202312051300_Tours-THROMBMICS-Clot-Segmentation.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36c99905-ed78-4dbb-9cee-b32fd79678cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_label_img = tio.LabelMap(\"G:\\\\Data_Booster\\\\data_ETIS_781\\\\MASK\\\\2018-104_01-10117-D0MR_S7_202312051300_Tours-THROMBMICS-Clot-Segmentation.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dca3dea4-71ed-4c4b-a546-72577bbe78eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_scalar_img = tio.Resample(target=reference_img)(mask_scalar_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39bd8c04-5268-4c1b-bcce-5ffe979bf6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_label_img = tio.Resample(target=reference_img)(mask_label_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f109783-d28f-4fc1-8da0-b8c614006630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "         [[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "         [[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "         [[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "         [[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_scalar_img.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "910f8f8f-0858-4518-9033-a616740e187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_array = nib.load(\"G:\\\\Data_Booster\\\\data_ETIS_781\\\\MASK\\\\2018-104_01-10117-D0MR_S7_202312051300_Tours-THROMBMICS-Clot-Segmentation.nii.gz\").get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "85d899a4-fda6-4d35-9226-c5760d88530a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([53081937,     2223], dtype=int64))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(normalized_label_img, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7885ce21-02cb-4fee-8cbd-3327749e4134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
